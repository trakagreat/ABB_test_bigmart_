{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5736996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b539dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train_v9rqX0R.csv\", low_memory=False)\n",
    "test = pd.read_csv(\"data/test_AbJTz2l.csv\", low_memory=False)\n",
    "test['Item_Outlet_Sales'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30679231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4092f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Item_Fat_Content'] = data['Item_Fat_Content'].replace({\n",
    "    'LF': 'Low Fat', 'low fat': 'Low Fat', 'reg': 'Regular'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d18ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Item_Identifier",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Item_Weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Item_Fat_Content",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Item_Visibility",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Item_Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Item_MRP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Outlet_Identifier",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Outlet_Establishment_Year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Outlet_Size",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Outlet_Location_Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Outlet_Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Item_Outlet_Sales",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "db2dc867-50ca-486c-ad62-d8734e70238e",
       "rows": [
        [
         "0",
         "FDA15",
         "9.3",
         "Low Fat",
         "0.016047301",
         "Dairy",
         "249.8092",
         "OUT049",
         "1999",
         "Medium",
         "Tier 1",
         "Supermarket Type1",
         "3735.138"
        ],
        [
         "1",
         "DRC01",
         "5.92",
         "Regular",
         "0.019278216",
         "Soft Drinks",
         "48.2692",
         "OUT018",
         "2009",
         "Medium",
         "Tier 3",
         "Supermarket Type2",
         "443.4228"
        ],
        [
         "2",
         "FDN15",
         "17.5",
         "Low Fat",
         "0.016760075",
         "Meat",
         "141.618",
         "OUT049",
         "1999",
         "Medium",
         "Tier 1",
         "Supermarket Type1",
         "2097.27"
        ],
        [
         "3",
         "FDX07",
         "19.2",
         "Regular",
         "0.0",
         "Fruits and Vegetables",
         "182.095",
         "OUT010",
         "1998",
         null,
         "Tier 3",
         "Grocery Store",
         "732.38"
        ],
        [
         "4",
         "NCD19",
         "8.93",
         "Low Fat",
         "0.0",
         "Household",
         "53.8614",
         "OUT013",
         "1987",
         "High",
         "Tier 3",
         "Supermarket Type1",
         "994.7052"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.30</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRC01</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>Soft Drinks</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>443.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDN15</td>\n",
       "      <td>17.50</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>Meat</td>\n",
       "      <td>141.6180</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2097.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDX07</td>\n",
       "      <td>19.20</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fruits and Vegetables</td>\n",
       "      <td>182.0950</td>\n",
       "      <td>OUT010</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Grocery Store</td>\n",
       "      <td>732.3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCD19</td>\n",
       "      <td>8.93</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Household</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>High</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>994.7052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility  \\\n",
       "0           FDA15         9.30          Low Fat         0.016047   \n",
       "1           DRC01         5.92          Regular         0.019278   \n",
       "2           FDN15        17.50          Low Fat         0.016760   \n",
       "3           FDX07        19.20          Regular         0.000000   \n",
       "4           NCD19         8.93          Low Fat         0.000000   \n",
       "\n",
       "               Item_Type  Item_MRP Outlet_Identifier  \\\n",
       "0                  Dairy  249.8092            OUT049   \n",
       "1            Soft Drinks   48.2692            OUT018   \n",
       "2                   Meat  141.6180            OUT049   \n",
       "3  Fruits and Vegetables  182.0950            OUT010   \n",
       "4              Household   53.8614            OUT013   \n",
       "\n",
       "   Outlet_Establishment_Year Outlet_Size Outlet_Location_Type  \\\n",
       "0                       1999      Medium               Tier 1   \n",
       "1                       2009      Medium               Tier 3   \n",
       "2                       1999      Medium               Tier 1   \n",
       "3                       1998         NaN               Tier 3   \n",
       "4                       1987        High               Tier 3   \n",
       "\n",
       "         Outlet_Type  Item_Outlet_Sales  \n",
       "0  Supermarket Type1          3735.1380  \n",
       "1  Supermarket Type2           443.4228  \n",
       "2  Supermarket Type1          2097.2700  \n",
       "3      Grocery Store           732.3800  \n",
       "4  Supermarket Type1           994.7052  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4db43d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14204 entries, 0 to 14203\n",
      "Data columns (total 12 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Item_Identifier            14204 non-null  object \n",
      " 1   Item_Weight                11765 non-null  float64\n",
      " 2   Item_Fat_Content           14204 non-null  object \n",
      " 3   Item_Visibility            14204 non-null  float64\n",
      " 4   Item_Type                  14204 non-null  object \n",
      " 5   Item_MRP                   14204 non-null  float64\n",
      " 6   Outlet_Identifier          14204 non-null  object \n",
      " 7   Outlet_Establishment_Year  14204 non-null  int64  \n",
      " 8   Outlet_Size                10188 non-null  object \n",
      " 9   Outlet_Location_Type       14204 non-null  object \n",
      " 10  Outlet_Type                14204 non-null  object \n",
      " 11  Item_Outlet_Sales          8523 non-null   float64\n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91406f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "95f894cc-4f98-460a-b4e1-32e6b4d25d5b",
       "rows": [
        [
         "Item_Identifier",
         "0"
        ],
        [
         "Item_Weight",
         "2439"
        ],
        [
         "Item_Fat_Content",
         "0"
        ],
        [
         "Item_Visibility",
         "0"
        ],
        [
         "Item_Type",
         "0"
        ],
        [
         "Item_MRP",
         "0"
        ],
        [
         "Outlet_Identifier",
         "0"
        ],
        [
         "Outlet_Establishment_Year",
         "0"
        ],
        [
         "Outlet_Size",
         "4016"
        ],
        [
         "Outlet_Location_Type",
         "0"
        ],
        [
         "Outlet_Type",
         "0"
        ],
        [
         "Item_Outlet_Sales",
         "5681"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 12
       }
      },
      "text/plain": [
       "Item_Identifier                 0\n",
       "Item_Weight                  2439\n",
       "Item_Fat_Content                0\n",
       "Item_Visibility                 0\n",
       "Item_Type                       0\n",
       "Item_MRP                        0\n",
       "Outlet_Identifier               0\n",
       "Outlet_Establishment_Year       0\n",
       "Outlet_Size                  4016\n",
       "Outlet_Location_Type            0\n",
       "Outlet_Type                     0\n",
       "Item_Outlet_Sales            5681\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71774f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing Item_Weight with mean\n",
    "data['Item_Weight'].fillna(data['Item_Weight'].median(), inplace=True)\n",
    "data['Item_Weight'].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9941e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Price_per_kg'] = data['Item_MRP'] / data['Item_Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df64d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Medium', nan, 'High', 'Small'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Outlet_Size'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d41dd5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Supermarket Type1', 'Supermarket Type2', 'Grocery Store',\n",
       "       'Supermarket Type3'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Outlet_Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94373054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fill missing Outlet_Size using mode per Outlet_Type\n",
    "data['Outlet_Size'].fillna(data.groupby('Outlet_Type')['Outlet_Size'].transform(lambda x: x.mode()[0]), inplace=True)\n",
    "data['Outlet_Size'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52562c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create New Feature: Outlet_Years\n",
    "data['Outlet_Years'] = 2025 - data['Outlet_Establishment_Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a0f634e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Dairy', 'Soft Drinks', 'Meat', 'Fruits and Vegetables',\n",
       "       'Household', 'Baking Goods', 'Snack Foods', 'Frozen Foods',\n",
       "       'Breakfast', 'Health and Hygiene', 'Hard Drinks', 'Canned',\n",
       "       'Breads', 'Starchy Foods', 'Others', 'Seafood'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Item_Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc66aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make groups for item type\n",
    "perishables = ['Dairy', 'Fruits and Vegetables', 'Meat', 'Frozen Foods', 'Seafood', 'Breads', 'Breakfast']\n",
    "non_perishables = ['Canned', 'Baking Goods', 'Starchy Foods', 'Household', 'Health and Hygiene']\n",
    "consumables = ['Snack Foods', 'Soft Drinks', 'Hard Drinks']\n",
    "other = ['Others']\n",
    "\n",
    "def group_item_type(x):\n",
    "    if x in perishables:\n",
    "        return 'Perishable'\n",
    "    elif x in non_perishables:\n",
    "        return 'Non-Perishable'\n",
    "    elif x in consumables:\n",
    "        return 'Consumables'\n",
    "    else:\n",
    "        return 'Others'\n",
    "\n",
    "data['Item_Category_Grouped'] = data['Item_Type'].apply(group_item_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6728458c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Perishable', 'Consumables', 'Non-Perishable', 'Others'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Item_Category_Grouped'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78bcf2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Low Fat', 'Regular'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Item_Fat_Content'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72ebf79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Tier 1', 'Tier 3', 'Tier 2'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Outlet_Location_Type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02548da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0808f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create New Feature: Item_Visibility_MeanRatio\n",
    "data['Item_Visibility_MeanRatio'] = data['Item_Visibility'] / data.groupby('Item_Identifier')['Item_Visibility'].transform('mean')\n",
    "data['Item_Visibility_MeanRatio'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "data['Item_Visibility_MeanRatio'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76989e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Categorical Variables\n",
    "le = LabelEncoder()\n",
    "categorical_cols = ['Item_Fat_Content', 'Outlet_Location_Type', 'Outlet_Size', 'Outlet_Type', 'Item_Category_Grouped']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    data[col] = le.fit_transform(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90cee37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Item_Identifier",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Item_Weight",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Item_Fat_Content",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Item_Visibility",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Item_Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Item_MRP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Outlet_Identifier",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Outlet_Establishment_Year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Outlet_Size",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Outlet_Location_Type",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Outlet_Type",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Item_Outlet_Sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Price_per_kg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Outlet_Years",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Item_Category_Grouped",
         "rawType": "int32",
         "type": "integer"
        },
        {
         "name": "Item_Visibility_MeanRatio",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8a76de23-70b0-42bb-9d65-61667b21b033",
       "rows": [
        [
         "0",
         "FDA15",
         "9.3",
         "0",
         "0.016047301",
         "Dairy",
         "249.8092",
         "OUT049",
         "1999",
         "1",
         "0",
         "1",
         "3735.138",
         "26.86120430107527",
         "26",
         "3",
         "0.9310779543761085"
        ],
        [
         "1",
         "DRC01",
         "5.92",
         "1",
         "0.019278216",
         "Soft Drinks",
         "48.2692",
         "OUT018",
         "2009",
         "1",
         "2",
         "2",
         "443.4228",
         "8.15358108108108",
         "16",
         "0",
         "0.9334195208758377"
        ],
        [
         "2",
         "FDN15",
         "17.5",
         "0",
         "0.016760075",
         "Meat",
         "141.618",
         "OUT049",
         "1999",
         "1",
         "0",
         "1",
         "2097.27",
         "8.092457142857143",
         "26",
         "3",
         "0.9600687539048446"
        ],
        [
         "3",
         "FDX07",
         "19.2",
         "1",
         "0.0",
         "Fruits and Vegetables",
         "182.095",
         "OUT010",
         "1998",
         "2",
         "2",
         "0",
         "732.38",
         "9.484114583333334",
         "27",
         "3",
         "0.0"
        ],
        [
         "4",
         "NCD19",
         "8.93",
         "0",
         "0.0",
         "Household",
         "53.8614",
         "OUT013",
         "1987",
         "0",
         "2",
         "1",
         "994.7052",
         "6.031511758118701",
         "38",
         "1",
         "0.0"
        ],
        [
         "5",
         "FDP36",
         "10.395",
         "1",
         "0.0",
         "Baking Goods",
         "51.4008",
         "OUT018",
         "2009",
         "1",
         "2",
         "2",
         "556.6088",
         "4.944761904761904",
         "16",
         "1",
         "0.0"
        ],
        [
         "6",
         "FDO10",
         "13.65",
         "1",
         "0.012741089",
         "Snack Foods",
         "57.6588",
         "OUT013",
         "1987",
         "0",
         "2",
         "1",
         "343.5528",
         "4.224087912087912",
         "38",
         "0",
         "1.497197477651743"
        ],
        [
         "7",
         "FDP10",
         "12.6",
         "0",
         "0.127469857",
         "Snack Foods",
         "107.7622",
         "OUT027",
         "1985",
         "1",
         "2",
         "3",
         "4022.7636",
         "8.552555555555557",
         "40",
         "0",
         "0.8704928816397633"
        ],
        [
         "8",
         "FDH17",
         "16.2",
         "1",
         "0.016687114",
         "Frozen Foods",
         "96.9726",
         "OUT045",
         "2002",
         "2",
         "1",
         "1",
         "1076.5986",
         "5.985962962962963",
         "23",
         "3",
         "0.9241603619293527"
        ],
        [
         "9",
         "FDU28",
         "19.2",
         "1",
         "0.09444959",
         "Frozen Foods",
         "187.8214",
         "OUT017",
         "2007",
         "2",
         "1",
         "1",
         "4710.535",
         "9.782364583333335",
         "18",
         "3",
         "0.9639830398302507"
        ],
        [
         "10",
         "FDY07",
         "11.8",
         "0",
         "0.0",
         "Fruits and Vegetables",
         "45.5402",
         "OUT049",
         "1999",
         "1",
         "0",
         "1",
         "1516.0266",
         "3.859338983050847",
         "26",
         "3",
         "0.0"
        ],
        [
         "11",
         "FDA03",
         "18.5",
         "1",
         "0.045463773",
         "Dairy",
         "144.1102",
         "OUT046",
         "1997",
         "2",
         "0",
         "1",
         "2187.153",
         "7.78974054054054",
         "28",
         "3",
         "1.0366952071584155"
        ],
        [
         "12",
         "FDX32",
         "15.1",
         "1",
         "0.1000135",
         "Fruits and Vegetables",
         "145.4786",
         "OUT049",
         "1999",
         "1",
         "0",
         "1",
         "1589.2646",
         "9.634344370860928",
         "26",
         "3",
         "1.026359813124039"
        ],
        [
         "13",
         "FDS46",
         "17.6",
         "1",
         "0.047257328",
         "Snack Foods",
         "119.6782",
         "OUT046",
         "1997",
         "2",
         "0",
         "1",
         "2145.2076",
         "6.799897727272727",
         "28",
         "0",
         "0.922289906298101"
        ],
        [
         "14",
         "FDF32",
         "16.35",
         "0",
         "0.0680243",
         "Fruits and Vegetables",
         "196.4426",
         "OUT013",
         "1987",
         "0",
         "2",
         "1",
         "1977.426",
         "12.014837920489295",
         "38",
         "3",
         "1.1713313653743491"
        ],
        [
         "15",
         "FDP49",
         "9.0",
         "1",
         "0.069088961",
         "Breakfast",
         "56.3614",
         "OUT046",
         "1997",
         "2",
         "0",
         "1",
         "1547.3192",
         "6.262377777777778",
         "28",
         "3",
         "1.0280725124413592"
        ],
        [
         "16",
         "NCB42",
         "11.8",
         "0",
         "0.008596051",
         "Health and Hygiene",
         "115.3492",
         "OUT018",
         "2009",
         "1",
         "2",
         "2",
         "1621.8888",
         "9.77535593220339",
         "16",
         "1",
         "1.0031395479206202"
        ],
        [
         "17",
         "FDP49",
         "9.0",
         "1",
         "0.069196376",
         "Breakfast",
         "54.3614",
         "OUT049",
         "1999",
         "1",
         "0",
         "1",
         "718.3982",
         "6.040155555555556",
         "26",
         "3",
         "1.0296708923753675"
        ],
        [
         "18",
         "DRI11",
         "12.6",
         "0",
         "0.034237682",
         "Hard Drinks",
         "113.2834",
         "OUT027",
         "1985",
         "1",
         "2",
         "3",
         "2303.668",
         "8.990746031746031",
         "40",
         "0",
         "0.8704928693111866"
        ],
        [
         "19",
         "FDU02",
         "13.35",
         "0",
         "0.10249212",
         "Dairy",
         "230.5352",
         "OUT035",
         "2004",
         "2",
         "1",
         "1",
         "2748.4224",
         "17.268554307116105",
         "21",
         "3",
         "0.922115518306462"
        ],
        [
         "20",
         "FDN22",
         "18.85",
         "1",
         "0.138190277",
         "Snack Foods",
         "250.8724",
         "OUT013",
         "1987",
         "0",
         "2",
         "1",
         "3775.086",
         "13.308880636604773",
         "38",
         "0",
         "1.1399044941112717"
        ],
        [
         "21",
         "FDW12",
         "12.6",
         "1",
         "0.035399923",
         "Baking Goods",
         "144.5444",
         "OUT027",
         "1985",
         "1",
         "2",
         "3",
         "4064.0432",
         "11.471777777777778",
         "40",
         "1",
         "0.9543090781180374"
        ],
        [
         "22",
         "NCB30",
         "14.6",
         "0",
         "0.025698134",
         "Household",
         "196.5084",
         "OUT035",
         "2004",
         "2",
         "1",
         "1",
         "1587.2672",
         "13.459479452054795",
         "21",
         "1",
         "0.8628943947372767"
        ],
        [
         "23",
         "FDC37",
         "12.6",
         "0",
         "0.057556998",
         "Baking Goods",
         "107.6938",
         "OUT019",
         "1985",
         "2",
         "0",
         "0",
         "214.3876",
         "8.547126984126985",
         "40",
         "1",
         "1.5315371875133976"
        ],
        [
         "24",
         "FDR28",
         "13.85",
         "1",
         "0.025896485",
         "Frozen Foods",
         "165.021",
         "OUT046",
         "1997",
         "2",
         "0",
         "1",
         "4078.025",
         "11.914873646209386",
         "28",
         "3",
         "0.9296326154440038"
        ],
        [
         "25",
         "NCD06",
         "13.0",
         "0",
         "0.099887103",
         "Household",
         "45.906",
         "OUT017",
         "2007",
         "2",
         "1",
         "1",
         "838.908",
         "3.531230769230769",
         "18",
         "1",
         "0.9275067672165533"
        ],
        [
         "26",
         "FDV10",
         "7.645",
         "1",
         "0.066693437",
         "Snack Foods",
         "42.3112",
         "OUT035",
         "2004",
         "2",
         "1",
         "1",
         "1065.28",
         "5.534493132766515",
         "21",
         "0",
         "1.0602351029427202"
        ],
        [
         "27",
         "DRJ59",
         "11.65",
         "0",
         "0.019356132",
         "Hard Drinks",
         "39.1164",
         "OUT013",
         "1987",
         "0",
         "2",
         "1",
         "308.9312",
         "3.3576309012875534",
         "38",
         "0",
         "1.0352775780298962"
        ],
        [
         "28",
         "FDE51",
         "5.925",
         "1",
         "0.161466534",
         "Dairy",
         "45.5086",
         "OUT010",
         "1998",
         "2",
         "2",
         "0",
         "178.4344",
         "7.680776371308017",
         "27",
         "3",
         "1.4445814400022103"
        ],
        [
         "29",
         "FDC14",
         "12.6",
         "1",
         "0.072221801",
         "Canned",
         "43.6454",
         "OUT019",
         "1985",
         "2",
         "0",
         "0",
         "125.8362",
         "3.463920634920635",
         "40",
         "1",
         "1.6790026812832133"
        ],
        [
         "30",
         "FDV38",
         "19.25",
         "0",
         "0.170348551",
         "Dairy",
         "55.7956",
         "OUT010",
         "1998",
         "2",
         "2",
         "0",
         "163.7868",
         "2.8984727272727273",
         "27",
         "3",
         "1.4641166675359907"
        ],
        [
         "31",
         "NCS17",
         "18.6",
         "0",
         "0.080829372",
         "Health and Hygiene",
         "96.4436",
         "OUT018",
         "2009",
         "1",
         "2",
         "2",
         "2741.7644",
         "5.185139784946236",
         "16",
         "1",
         "1.0031395405819457"
        ],
        [
         "32",
         "FDP33",
         "18.7",
         "0",
         "0.0",
         "Snack Foods",
         "256.6672",
         "OUT018",
         "2009",
         "1",
         "2",
         "2",
         "3068.0064",
         "13.72551871657754",
         "16",
         "0",
         "0.0"
        ],
        [
         "33",
         "FDO23",
         "17.85",
         "0",
         "0.0",
         "Breads",
         "93.1436",
         "OUT045",
         "2002",
         "2",
         "1",
         "1",
         "2174.5028",
         "5.218128851540616",
         "23",
         "3",
         "0.0"
        ],
        [
         "34",
         "DRH01",
         "17.5",
         "0",
         "0.097904029",
         "Soft Drinks",
         "174.8738",
         "OUT046",
         "1997",
         "2",
         "0",
         "1",
         "2085.2856",
         "9.992788571428571",
         "28",
         "0",
         "0.9222899199987085"
        ],
        [
         "35",
         "NCX29",
         "10.0",
         "0",
         "0.089291137",
         "Health and Hygiene",
         "146.7102",
         "OUT049",
         "1999",
         "1",
         "0",
         "1",
         "3791.0652",
         "14.671019999999999",
         "26",
         "1",
         "0.8760887635003262"
        ],
        [
         "36",
         "FDV20",
         "12.6",
         "1",
         "0.059511812",
         "Fruits and Vegetables",
         "128.0678",
         "OUT027",
         "1985",
         "1",
         "2",
         "3",
         "2797.6916",
         "10.164111111111112",
         "40",
         "3",
         "0.9544539631769143"
        ],
        [
         "37",
         "DRZ11",
         "8.85",
         "1",
         "0.113123893",
         "Soft Drinks",
         "122.5388",
         "OUT018",
         "2009",
         "1",
         "2",
         "2",
         "1609.9044",
         "13.84619209039548",
         "16",
         "0",
         "1.0031395349101433"
        ],
        [
         "38",
         "FDX10",
         "12.6",
         "1",
         "0.123111453",
         "Snack Foods",
         "36.9874",
         "OUT027",
         "1985",
         "1",
         "2",
         "3",
         "388.1614",
         "2.935507936507937",
         "40",
         "0",
         "1.0227997426879307"
        ],
        [
         "39",
         "FDB34",
         "12.6",
         "0",
         "0.026480954",
         "Snack Foods",
         "87.6198",
         "OUT027",
         "1985",
         "1",
         "2",
         "3",
         "2180.495",
         "6.953952380952381",
         "40",
         "0",
         "0.9251308290764402"
        ],
        [
         "40",
         "FDU02",
         "13.35",
         "0",
         "0.102511504",
         "Dairy",
         "230.6352",
         "OUT046",
         "1997",
         "2",
         "0",
         "1",
         "3435.528",
         "17.276044943820224",
         "28",
         "3",
         "0.9222899150035627"
        ],
        [
         "41",
         "FDK43",
         "9.8",
         "0",
         "0.02681843",
         "Meat",
         "126.002",
         "OUT013",
         "1987",
         "0",
         "2",
         "1",
         "2150.534",
         "12.857346938775509",
         "38",
         "3",
         "0.9215224160526706"
        ],
        [
         "42",
         "FDA46",
         "13.6",
         "0",
         "0.117818348",
         "Snack Foods",
         "192.9136",
         "OUT049",
         "1999",
         "1",
         "0",
         "1",
         "2527.3768",
         "14.184823529411766",
         "26",
         "0",
         "0.9310779364692868"
        ],
        [
         "43",
         "FDC02",
         "21.35",
         "0",
         "0.069102831",
         "Canned",
         "259.9278",
         "OUT018",
         "2009",
         "1",
         "2",
         "2",
         "6768.5228",
         "12.174604215456673",
         "16",
         "1",
         "0.9334195377024119"
        ],
        [
         "44",
         "FDL50",
         "12.15",
         "1",
         "0.042277867",
         "Canned",
         "126.5046",
         "OUT013",
         "1987",
         "0",
         "2",
         "1",
         "373.5138",
         "10.411901234567901",
         "38",
         "1",
         "0.9980787902561442"
        ],
        [
         "45",
         "FDM39",
         "6.42",
         "0",
         "0.089498926",
         "Dairy",
         "178.1002",
         "OUT010",
         "1998",
         "2",
         "2",
         "0",
         "358.2004",
         "27.74146417445483",
         "27",
         "3",
         "1.4641166676603148"
        ],
        [
         "46",
         "NCP05",
         "19.6",
         "0",
         "0.0",
         "Health and Hygiene",
         "153.3024",
         "OUT045",
         "2002",
         "2",
         "1",
         "1",
         "2428.8384",
         "7.821551020408163",
         "23",
         "1",
         "0.0"
        ],
        [
         "47",
         "FDV49",
         "10.0",
         "0",
         "0.025879577",
         "Canned",
         "265.2226",
         "OUT045",
         "2002",
         "2",
         "1",
         "1",
         "5815.0972",
         "26.52226",
         "23",
         "1",
         "0.9241603460120508"
        ],
        [
         "48",
         "FDL12",
         "15.85",
         "1",
         "0.121632721",
         "Baking Goods",
         "60.622",
         "OUT046",
         "1997",
         "2",
         "0",
         "1",
         "2576.646",
         "3.8247318611987384",
         "28",
         "1",
         "0.8747287915078714"
        ],
        [
         "49",
         "FDS02",
         "12.6",
         "1",
         "0.255394896",
         "Dairy",
         "196.8794",
         "OUT019",
         "1985",
         "2",
         "0",
         "0",
         "780.3176",
         "15.625349206349208",
         "40",
         "3",
         "2.03088772479079"
        ]
       ],
       "shape": {
        "columns": 16,
        "rows": 14204
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "      <th>Price_per_kg</th>\n",
       "      <th>Outlet_Years</th>\n",
       "      <th>Item_Category_Grouped</th>\n",
       "      <th>Item_Visibility_MeanRatio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3735.1380</td>\n",
       "      <td>26.861204</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>0.931078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRC01</td>\n",
       "      <td>5.92</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>Soft Drinks</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>443.4228</td>\n",
       "      <td>8.153581</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.933420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDN15</td>\n",
       "      <td>17.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>Meat</td>\n",
       "      <td>141.6180</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2097.2700</td>\n",
       "      <td>8.092457</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>0.960069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDX07</td>\n",
       "      <td>19.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fruits and Vegetables</td>\n",
       "      <td>182.0950</td>\n",
       "      <td>OUT010</td>\n",
       "      <td>1998</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>732.3800</td>\n",
       "      <td>9.484115</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCD19</td>\n",
       "      <td>8.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Household</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>994.7052</td>\n",
       "      <td>6.031512</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14199</th>\n",
       "      <td>FDB58</td>\n",
       "      <td>10.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>Snack Foods</td>\n",
       "      <td>141.3154</td>\n",
       "      <td>OUT046</td>\n",
       "      <td>1997</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.458610</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.874729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14200</th>\n",
       "      <td>FDD47</td>\n",
       "      <td>7.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142991</td>\n",
       "      <td>Starchy Foods</td>\n",
       "      <td>169.1448</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.255895</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.878292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14201</th>\n",
       "      <td>NCO17</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>Health and Hygiene</td>\n",
       "      <td>118.7440</td>\n",
       "      <td>OUT045</td>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.874400</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1.162245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14202</th>\n",
       "      <td>FDJ26</td>\n",
       "      <td>15.30</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Canned</td>\n",
       "      <td>214.6218</td>\n",
       "      <td>OUT017</td>\n",
       "      <td>2007</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.027569</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14203</th>\n",
       "      <td>FDU37</td>\n",
       "      <td>9.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.104720</td>\n",
       "      <td>Canned</td>\n",
       "      <td>79.7960</td>\n",
       "      <td>OUT045</td>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.399579</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1.029678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14204 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Item_Identifier  Item_Weight  Item_Fat_Content  Item_Visibility  \\\n",
       "0               FDA15         9.30                 0         0.016047   \n",
       "1               DRC01         5.92                 1         0.019278   \n",
       "2               FDN15        17.50                 0         0.016760   \n",
       "3               FDX07        19.20                 1         0.000000   \n",
       "4               NCD19         8.93                 0         0.000000   \n",
       "...               ...          ...               ...              ...   \n",
       "14199           FDB58        10.50                 1         0.013496   \n",
       "14200           FDD47         7.60                 1         0.142991   \n",
       "14201           NCO17        10.00                 0         0.073529   \n",
       "14202           FDJ26        15.30                 1         0.000000   \n",
       "14203           FDU37         9.50                 1         0.104720   \n",
       "\n",
       "                   Item_Type  Item_MRP Outlet_Identifier  \\\n",
       "0                      Dairy  249.8092            OUT049   \n",
       "1                Soft Drinks   48.2692            OUT018   \n",
       "2                       Meat  141.6180            OUT049   \n",
       "3      Fruits and Vegetables  182.0950            OUT010   \n",
       "4                  Household   53.8614            OUT013   \n",
       "...                      ...       ...               ...   \n",
       "14199            Snack Foods  141.3154            OUT046   \n",
       "14200          Starchy Foods  169.1448            OUT018   \n",
       "14201     Health and Hygiene  118.7440            OUT045   \n",
       "14202                 Canned  214.6218            OUT017   \n",
       "14203                 Canned   79.7960            OUT045   \n",
       "\n",
       "       Outlet_Establishment_Year  Outlet_Size  Outlet_Location_Type  \\\n",
       "0                           1999            1                     0   \n",
       "1                           2009            1                     2   \n",
       "2                           1999            1                     0   \n",
       "3                           1998            2                     2   \n",
       "4                           1987            0                     2   \n",
       "...                          ...          ...                   ...   \n",
       "14199                       1997            2                     0   \n",
       "14200                       2009            1                     2   \n",
       "14201                       2002            2                     1   \n",
       "14202                       2007            2                     1   \n",
       "14203                       2002            2                     1   \n",
       "\n",
       "       Outlet_Type  Item_Outlet_Sales  Price_per_kg  Outlet_Years  \\\n",
       "0                1          3735.1380     26.861204            26   \n",
       "1                2           443.4228      8.153581            16   \n",
       "2                1          2097.2700      8.092457            26   \n",
       "3                0           732.3800      9.484115            27   \n",
       "4                1           994.7052      6.031512            38   \n",
       "...            ...                ...           ...           ...   \n",
       "14199            1                NaN     13.458610            28   \n",
       "14200            2                NaN     22.255895            16   \n",
       "14201            1                NaN     11.874400            23   \n",
       "14202            1                NaN     14.027569            18   \n",
       "14203            1                NaN      8.399579            23   \n",
       "\n",
       "       Item_Category_Grouped  Item_Visibility_MeanRatio  \n",
       "0                          3                   0.931078  \n",
       "1                          0                   0.933420  \n",
       "2                          3                   0.960069  \n",
       "3                          3                   0.000000  \n",
       "4                          1                   0.000000  \n",
       "...                      ...                        ...  \n",
       "14199                      0                   0.874729  \n",
       "14200                      1                   0.878292  \n",
       "14201                      1                   1.162245  \n",
       "14202                      1                   0.000000  \n",
       "14203                      1                   1.029678  \n",
       "\n",
       "[14204 rows x 16 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1f53e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final list of features\n",
    "features = [\n",
    "    'Item_Weight', 'Item_Visibility', 'Item_MRP',\n",
    "    'Outlet_Years',\n",
    "    'Item_Fat_Content', 'Outlet_Location_Type',\n",
    "    'Outlet_Size', 'Outlet_Type', 'Item_Category_Grouped'\n",
    "]\n",
    "\n",
    "# features = [\n",
    "#     'Item_Weight', 'Item_Visibility', 'Item_MRP',\n",
    "#     'Outlet_Years',\n",
    "#     'Item_Fat_Content', 'Outlet_Location_Type',\n",
    "#     'Outlet_Size', 'Outlet_Type', 'Item_Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1a5e050",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_columns = ['Item_Identifier', 'Outlet_Identifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90174a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = data[~data['Item_Outlet_Sales'].isna()]\n",
    "test_df = data[data['Item_Outlet_Sales'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da1a1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### now further split train into train and validation\n",
    "\n",
    "X = train_df[features]\n",
    "y = train_df['Item_Outlet_Sales']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa46f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(alpha=1.0),\n",
    "    \"Lasso Regression\": Lasso(alpha=0.01),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.01, l1_ratio=0.5),\n",
    "    \n",
    "    \"Decision Tree\": DecisionTreeRegressor(max_depth=10, random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42),\n",
    "    \"Extra Trees\": ExtraTreesRegressor(n_estimators=100, random_state=42),\n",
    "    \n",
    "    \"AdaBoost\": AdaBoostRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, random_state=42),\n",
    "    \n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(n_neighbors=5),\n",
    "    \"Support Vector Regressor\": SVR(kernel='rbf', C=1.0, epsilon=0.2)\n",
    "}\n",
    "\n",
    "metrics = []\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    train_preds = model.predict(X_train)\n",
    "    val_preds = model.predict(X_val)\n",
    "    \n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, train_preds))\n",
    "    r2_train = r2_score(y_train, train_preds)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    r2_val = r2_score(y_val, val_preds)\n",
    "    \n",
    "    metrics.append({\n",
    "        'Model': name,\n",
    "        'RMSE_Train': round(rmse_train, 2),\n",
    "        'R2_Train': round(r2_train, 2),\n",
    "        'RMSE_val': round(rmse_val, 2),\n",
    "        'R2_val': round(r2_val, 2)\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "1914f206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "RMSE_Train",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "R2_Train",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "RMSE_val",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "R2_val",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "3c04d88a-55ef-4e0f-80d7-860a5a7a764d",
       "rows": [
        [
         "0",
         "Linear Regression",
         "1219.74",
         "0.5",
         "1142.94",
         "0.52"
        ],
        [
         "1",
         "Ridge Regression",
         "1219.75",
         "0.5",
         "1142.77",
         "0.52"
        ],
        [
         "2",
         "Lasso Regression",
         "1219.74",
         "0.5",
         "1142.93",
         "0.52"
        ],
        [
         "3",
         "ElasticNet",
         "1221.29",
         "0.5",
         "1142.59",
         "0.52"
        ],
        [
         "4",
         "Decision Tree",
         "959.41",
         "0.69",
         "1145.81",
         "0.52"
        ],
        [
         "5",
         "Random Forest",
         "910.2",
         "0.72",
         "1040.39",
         "0.6"
        ],
        [
         "6",
         "Extra Trees",
         "0.55",
         "1.0",
         "1122.46",
         "0.54"
        ],
        [
         "7",
         "AdaBoost",
         "1135.18",
         "0.56",
         "1088.44",
         "0.56"
        ],
        [
         "8",
         "Gradient Boosting",
         "948.99",
         "0.7",
         "1049.17",
         "0.6"
        ],
        [
         "9",
         "XGBoost",
         "900.29",
         "0.73",
         "1055.07",
         "0.59"
        ],
        [
         "10",
         "K-Nearest Neighbors",
         "1060.82",
         "0.62",
         "1281.32",
         "0.4"
        ],
        [
         "11",
         "Support Vector Regressor",
         "1552.58",
         "0.19",
         "1469.13",
         "0.21"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 12
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE_Train</th>\n",
       "      <th>R2_Train</th>\n",
       "      <th>RMSE_val</th>\n",
       "      <th>R2_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>1219.74</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1142.94</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>1219.75</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1142.77</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Regression</td>\n",
       "      <td>1219.74</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1142.93</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>1221.29</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1142.59</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>959.41</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1145.81</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>910.20</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1040.39</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.55</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1122.46</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>1135.18</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1088.44</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>948.99</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1049.17</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>900.29</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1055.07</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>1060.82</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1281.32</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Support Vector Regressor</td>\n",
       "      <td>1552.58</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1469.13</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Model  RMSE_Train  R2_Train  RMSE_val  R2_val\n",
       "0          Linear Regression     1219.74      0.50   1142.94    0.52\n",
       "1           Ridge Regression     1219.75      0.50   1142.77    0.52\n",
       "2           Lasso Regression     1219.74      0.50   1142.93    0.52\n",
       "3                 ElasticNet     1221.29      0.50   1142.59    0.52\n",
       "4              Decision Tree      959.41      0.69   1145.81    0.52\n",
       "5              Random Forest      910.20      0.72   1040.39    0.60\n",
       "6                Extra Trees        0.55      1.00   1122.46    0.54\n",
       "7                   AdaBoost     1135.18      0.56   1088.44    0.56\n",
       "8          Gradient Boosting      948.99      0.70   1049.17    0.60\n",
       "9                    XGBoost      900.29      0.73   1055.07    0.59\n",
       "10       K-Nearest Neighbors     1060.82      0.62   1281.32    0.40\n",
       "11  Support Vector Regressor     1552.58      0.19   1469.13    0.21"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "b2ec9926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 150 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2084, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Low Fat'\n\n--------------------------------------------------------------------------------\n120 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2084, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Regular'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[543], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m X_full_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([X_train, X_val], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     32\u001b[0m y_full_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([y_train, y_val], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_full_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_full_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Best model and params\u001b[39;00m\n\u001b[0;32m     37\u001b[0m best_rf \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1770\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 150 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2084, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Low Fat'\n\n--------------------------------------------------------------------------------\n120 fits failed with the following error:\nTraceback (most recent call last):\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 879, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 185, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"d:\\Anconda_new\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2084, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Regular'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define parameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Instantiate the base model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,  # number of combinations to try\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit on training set\n",
    "\n",
    "# Combine X_train and X_val\n",
    "X_full_train = pd.concat([X_train, X_val], axis=0)\n",
    "y_full_train = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "random_search.fit(X_full_train, y_full_train)\n",
    "\n",
    "# Best model and params\n",
    "best_rf = random_search.best_estimator_\n",
    "print(\"\\nBest Parameters found:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Evaluate on validation set\n",
    "train_preds = best_rf.predict(X_train)\n",
    "val_preds = best_rf.predict(X_val)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, train_preds))\n",
    "r2_train = r2_score(y_train, train_preds)\n",
    "rmse_val = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "r2_val = r2_score(y_val, val_preds)\n",
    "\n",
    "print(f\"\\n Tuned Random Forest Performance:\")\n",
    "print(f\"RMSE Train: {rmse_train:.2f}, R2 Train: {r2_train:.2f}\")\n",
    "print(f\"RMSE Validation: {rmse_val:.2f}, R2 Validation: {r2_val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd2efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitro\\AppData\\Local\\Temp\\ipykernel_16200\\1155128571.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['Item_Outlet_Sales'] = final_test_preds\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set using the best tuned model\n",
    "final_test_preds = best_rf.predict(test_df[features])\n",
    "\n",
    "# Attach predictions to test set\n",
    "test_df['Item_Outlet_Sales'] = final_test_preds\n",
    "\n",
    "# Create submission file\n",
    "submission = test_df[['Item_Identifier', 'Outlet_Identifier', 'Item_Outlet_Sales']]\n",
    "submission.to_csv(\"BigMart_Prediction_Submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82995863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "\n",
      "Best Parameters (XGBoost):\n",
      "{'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 1, 'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.05, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "    'reg_lambda': [1, 1.5, 2, 3]\n",
    "}\n",
    "\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_grid_xgb,\n",
    "    n_iter=30,\n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search_xgb.fit(X_full_train, y_full_train)\n",
    "\n",
    "best_xgb = random_search_xgb.best_estimator_\n",
    "print(\"\\nBest Parameters (XGBoost):\")\n",
    "print(random_search_xgb.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4217337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Submission file 'submission_xgboost.csv' saved.\n"
     ]
    }
   ],
   "source": [
    "# Predict on the final test set using tuned XGBoost model\n",
    "test_preds = best_xgb.predict(test_df[features])\n",
    "\n",
    "# Prepare submission DataFrame\n",
    "submission = test[['Item_Identifier', 'Outlet_Identifier']].copy()\n",
    "submission['Item_Outlet_Sales'] = test_preds\n",
    "\n",
    "\n",
    "submission['Item_Outlet_Sales'].clip(lower=0, inplace=True)\n",
    "# Save to CSV\n",
    "submission.to_csv('submission_xgboost.csv', index=False)\n",
    "\n",
    "print(\"âœ… Submission file 'submission_xgboost.csv' saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ae509",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "4cb8b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "f40362c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define the preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', StandardScaler(), [col for col in X_train.columns if col not in categorical_cols])\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "f095782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test  = test_df[features]\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Fit on training data\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "X_val_encoded = preprocessor.transform(X_val)\n",
    "X_test_encoded = preprocessor.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# X_train_scaled = scaler.fit_transform(X_full_train)\n",
    "# # X_val_scaled = scaler.transform(X_val)\n",
    "# # X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "ce8ae221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anconda_new\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.8,        # reduce by half\n",
    "    patience=10,        # if val_loss doesnâ€™t improve for 5 epochs\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=X_train_encoded.shape[1], activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    # BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),    loss=rmse,     \n",
    "    metrics=[rmse] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "2f2a7479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1121.9515 - rmse: 1121.9547 - val_loss: 1013.9398 - val_rmse: 1019.6498\n",
      "Epoch 2/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1115.2325 - rmse: 1115.2349 - val_loss: 1013.8835 - val_rmse: 1019.5859\n",
      "Epoch 3/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1099.7574 - rmse: 1099.7596 - val_loss: 1013.7935 - val_rmse: 1019.4826\n",
      "Epoch 4/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1140.2422 - rmse: 1140.2428 - val_loss: 1013.6262 - val_rmse: 1019.3116\n",
      "Epoch 5/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1107.0573 - rmse: 1107.0568 - val_loss: 1014.0966 - val_rmse: 1019.8278\n",
      "Epoch 6/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1144.4458 - rmse: 1144.4426 - val_loss: 1013.5617 - val_rmse: 1019.2263\n",
      "Epoch 7/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1103.2423 - rmse: 1103.2390 - val_loss: 1013.5959 - val_rmse: 1019.2758\n",
      "Epoch 8/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1115.5511 - rmse: 1115.5521 - val_loss: 1013.9140 - val_rmse: 1019.6241\n",
      "Epoch 9/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1095.4524 - rmse: 1095.4558 - val_loss: 1014.1053 - val_rmse: 1019.8438\n",
      "Epoch 10/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1124.2162 - rmse: 1124.2156 - val_loss: 1013.8392 - val_rmse: 1019.5341\n",
      "Epoch 11/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.3984 - rmse: 1120.4016 - val_loss: 1013.7364 - val_rmse: 1019.4144\n",
      "Epoch 12/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1109.4858 - rmse: 1109.4805 - val_loss: 1013.8640 - val_rmse: 1019.5631\n",
      "Epoch 13/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1132.2988 - rmse: 1132.3048 - val_loss: 1013.8516 - val_rmse: 1019.5427\n",
      "Epoch 14/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1152.1381 - rmse: 1152.1349 - val_loss: 1013.6618 - val_rmse: 1019.3280\n",
      "Epoch 15/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1114.6687 - rmse: 1114.6731 - val_loss: 1013.6623 - val_rmse: 1019.3235\n",
      "Epoch 16/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1132.9379 - rmse: 1132.9412 - val_loss: 1013.7500 - val_rmse: 1019.4123\n",
      "Epoch 17/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1094.9335 - rmse: 1094.9319 - val_loss: 1013.6074 - val_rmse: 1019.2632\n",
      "Epoch 18/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1137.5609 - rmse: 1137.5592 - val_loss: 1013.4788 - val_rmse: 1019.1198\n",
      "Epoch 19/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1095.0869 - rmse: 1095.0916 - val_loss: 1013.9109 - val_rmse: 1019.6002\n",
      "Epoch 20/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1144.4229 - rmse: 1144.4213 - val_loss: 1013.8723 - val_rmse: 1019.5745\n",
      "Epoch 21/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1121.7528 - rmse: 1121.7551 - val_loss: 1013.8268 - val_rmse: 1019.5176\n",
      "Epoch 22/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1122.9321 - rmse: 1122.9377 - val_loss: 1013.7840 - val_rmse: 1019.4720\n",
      "Epoch 23/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1108.6328 - rmse: 1108.6292 - val_loss: 1013.8604 - val_rmse: 1019.5641\n",
      "Epoch 24/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.6309 - rmse: 1123.6263 - val_loss: 1013.7581 - val_rmse: 1019.4491\n",
      "Epoch 25/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1118.0771 - rmse: 1118.0765 - val_loss: 1014.0508 - val_rmse: 1019.7767\n",
      "Epoch 26/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1110.3708 - rmse: 1110.3719 - val_loss: 1013.8616 - val_rmse: 1019.5740\n",
      "Epoch 27/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1122.5140 - rmse: 1122.5161 - val_loss: 1013.7404 - val_rmse: 1019.4432\n",
      "Epoch 28/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1139.5591 - rmse: 1139.5576 - val_loss: 1013.5706 - val_rmse: 1019.2418\n",
      "Epoch 29/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1113.1816 - rmse: 1113.1843 - val_loss: 1013.8021 - val_rmse: 1019.5125\n",
      "Epoch 30/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1119.3076 - rmse: 1119.3173 - val_loss: 1013.9304 - val_rmse: 1019.6427\n",
      "Epoch 31/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.0979 - rmse: 1123.0931 - val_loss: 1013.6843 - val_rmse: 1019.3691\n",
      "Epoch 32/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1098.9963 - rmse: 1099.0057 - val_loss: 1013.5768 - val_rmse: 1019.2502\n",
      "Epoch 33/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.8344 - rmse: 1120.8368 - val_loss: 1013.6163 - val_rmse: 1019.3033\n",
      "Epoch 34/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1111.4048 - rmse: 1111.4050 - val_loss: 1013.6919 - val_rmse: 1019.3891\n",
      "Epoch 35/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1116.2653 - rmse: 1116.2703 - val_loss: 1013.5532 - val_rmse: 1019.2292\n",
      "Epoch 36/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1106.2285 - rmse: 1106.2296 - val_loss: 1014.1583 - val_rmse: 1019.9078\n",
      "Epoch 37/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1134.0837 - rmse: 1134.0862 - val_loss: 1013.7025 - val_rmse: 1019.4047\n",
      "Epoch 38/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1110.2407 - rmse: 1110.2388 - val_loss: 1013.8817 - val_rmse: 1019.6051\n",
      "Epoch 39/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1124.5620 - rmse: 1124.5614 - val_loss: 1013.5728 - val_rmse: 1019.2552\n",
      "Epoch 40/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1126.6049 - rmse: 1126.6069 - val_loss: 1013.6278 - val_rmse: 1019.3087\n",
      "Epoch 41/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1134.3416 - rmse: 1134.3535 - val_loss: 1013.6122 - val_rmse: 1019.3115\n",
      "Epoch 42/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1109.0631 - rmse: 1109.0573 - val_loss: 1013.5808 - val_rmse: 1019.2706\n",
      "Epoch 43/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.5480 - rmse: 1120.5519 - val_loss: 1013.5829 - val_rmse: 1019.2657\n",
      "Epoch 44/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1114.4089 - rmse: 1114.4077 - val_loss: 1013.6874 - val_rmse: 1019.3897\n",
      "Epoch 45/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1101.1951 - rmse: 1101.1987 - val_loss: 1013.7650 - val_rmse: 1019.4633\n",
      "Epoch 46/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1094.7505 - rmse: 1094.7499 - val_loss: 1013.7295 - val_rmse: 1019.4238\n",
      "Epoch 47/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1114.4586 - rmse: 1114.4609 - val_loss: 1013.8128 - val_rmse: 1019.5208\n",
      "Epoch 48/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1130.9066 - rmse: 1130.9108 - val_loss: 1013.5605 - val_rmse: 1019.2160\n",
      "Epoch 49/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1140.9781 - rmse: 1140.9736 - val_loss: 1013.3801 - val_rmse: 1019.0040\n",
      "Epoch 50/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1131.9724 - rmse: 1131.9733 - val_loss: 1013.6907 - val_rmse: 1019.3708\n",
      "Epoch 51/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1126.9480 - rmse: 1126.9508 - val_loss: 1013.7300 - val_rmse: 1019.4159\n",
      "Epoch 52/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1095.8958 - rmse: 1095.8972 - val_loss: 1013.6693 - val_rmse: 1019.3507\n",
      "Epoch 53/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1146.2825 - rmse: 1146.2802 - val_loss: 1013.4332 - val_rmse: 1019.0850\n",
      "Epoch 54/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1130.1837 - rmse: 1130.1805 - val_loss: 1013.6935 - val_rmse: 1019.3742\n",
      "Epoch 55/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1121.7682 - rmse: 1121.7675 - val_loss: 1013.7464 - val_rmse: 1019.4261\n",
      "Epoch 56/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1113.8502 - rmse: 1113.8479 - val_loss: 1013.9429 - val_rmse: 1019.6484\n",
      "Epoch 57/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1131.3121 - rmse: 1131.3121 - val_loss: 1013.6692 - val_rmse: 1019.3408\n",
      "Epoch 58/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.5986 - rmse: 1120.5970 - val_loss: 1013.6505 - val_rmse: 1019.3143\n",
      "Epoch 59/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1110.7621 - rmse: 1110.7710 - val_loss: 1013.5614 - val_rmse: 1019.2062\n",
      "Epoch 60/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1104.9915 - rmse: 1104.9952 - val_loss: 1013.7769 - val_rmse: 1019.4594\n",
      "Epoch 61/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1116.9587 - rmse: 1116.9601 - val_loss: 1013.6194 - val_rmse: 1019.2892\n",
      "Epoch 62/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1139.1479 - rmse: 1139.1531 - val_loss: 1013.7985 - val_rmse: 1019.4786\n",
      "Epoch 63/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1127.6582 - rmse: 1127.6573 - val_loss: 1013.7463 - val_rmse: 1019.4221\n",
      "Epoch 64/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1077.9832 - rmse: 1077.9897 - val_loss: 1013.6432 - val_rmse: 1019.3153\n",
      "Epoch 65/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1124.9539 - rmse: 1124.9490 - val_loss: 1013.6168 - val_rmse: 1019.2884\n",
      "Epoch 66/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1129.7848 - rmse: 1129.7844 - val_loss: 1013.5593 - val_rmse: 1019.2266\n",
      "Epoch 67/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1103.9384 - rmse: 1103.9436 - val_loss: 1013.7587 - val_rmse: 1019.4554\n",
      "Epoch 68/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1096.9291 - rmse: 1096.9309 - val_loss: 1013.5135 - val_rmse: 1019.1676\n",
      "Epoch 69/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1097.6536 - rmse: 1097.6550 - val_loss: 1013.5783 - val_rmse: 1019.2432\n",
      "Epoch 70/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1138.7076 - rmse: 1138.7056 - val_loss: 1013.6509 - val_rmse: 1019.3262\n",
      "Epoch 71/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.1711 - rmse: 1123.1655 - val_loss: 1013.6483 - val_rmse: 1019.3198\n",
      "Epoch 72/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1126.3007 - rmse: 1126.3025 - val_loss: 1013.2650 - val_rmse: 1018.8893\n",
      "Epoch 73/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1113.6552 - rmse: 1113.6554 - val_loss: 1013.5334 - val_rmse: 1019.1937\n",
      "Epoch 74/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1097.0000 - rmse: 1097.0028 - val_loss: 1013.8568 - val_rmse: 1019.5657\n",
      "Epoch 75/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1118.6923 - rmse: 1118.6871 - val_loss: 1013.7946 - val_rmse: 1019.4971\n",
      "Epoch 76/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.3224 - rmse: 1123.3206 - val_loss: 1013.9156 - val_rmse: 1019.6285\n",
      "Epoch 77/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1129.5721 - rmse: 1129.5753 - val_loss: 1013.7419 - val_rmse: 1019.4482\n",
      "Epoch 78/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1113.3088 - rmse: 1113.3057 - val_loss: 1013.7411 - val_rmse: 1019.4481\n",
      "Epoch 79/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1115.1755 - rmse: 1115.1777 - val_loss: 1013.7572 - val_rmse: 1019.4644\n",
      "Epoch 80/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1122.2112 - rmse: 1122.2114 - val_loss: 1013.6666 - val_rmse: 1019.3591\n",
      "Epoch 81/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1143.6039 - rmse: 1143.6067 - val_loss: 1013.6567 - val_rmse: 1019.3418\n",
      "Epoch 82/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1114.2229 - rmse: 1114.2279 - val_loss: 1013.7655 - val_rmse: 1019.4635\n",
      "Epoch 83/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1153.5934 - rmse: 1153.5958 - val_loss: 1013.6198 - val_rmse: 1019.3094\n",
      "Epoch 84/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1126.0354 - rmse: 1126.0450 - val_loss: 1013.5905 - val_rmse: 1019.2743\n",
      "Epoch 85/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1099.2931 - rmse: 1099.2870 - val_loss: 1013.5865 - val_rmse: 1019.2758\n",
      "Epoch 86/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1127.3793 - rmse: 1127.3833 - val_loss: 1013.9078 - val_rmse: 1019.6415\n",
      "Epoch 87/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1099.9288 - rmse: 1099.9342 - val_loss: 1013.8382 - val_rmse: 1019.5692\n",
      "Epoch 88/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1123.4137 - rmse: 1123.4159 - val_loss: 1013.5794 - val_rmse: 1019.2807\n",
      "Epoch 89/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1132.2285 - rmse: 1132.2296 - val_loss: 1013.7862 - val_rmse: 1019.5121\n",
      "Epoch 90/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1126.1492 - rmse: 1126.1482 - val_loss: 1013.6509 - val_rmse: 1019.3481\n",
      "Epoch 91/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1119.3826 - rmse: 1119.3894 - val_loss: 1013.7731 - val_rmse: 1019.4829\n",
      "Epoch 92/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1149.5426 - rmse: 1149.5405 - val_loss: 1013.6981 - val_rmse: 1019.4139\n",
      "Epoch 93/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.9250 - rmse: 1120.9247 - val_loss: 1013.3474 - val_rmse: 1019.0103\n",
      "Epoch 94/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1132.2129 - rmse: 1132.2095 - val_loss: 1013.6037 - val_rmse: 1019.3073\n",
      "Epoch 95/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1110.0449 - rmse: 1110.0450 - val_loss: 1013.7827 - val_rmse: 1019.5070\n",
      "Epoch 96/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.7640 - rmse: 1120.7686 - val_loss: 1013.7628 - val_rmse: 1019.4813\n",
      "Epoch 97/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1129.3688 - rmse: 1129.3678 - val_loss: 1013.7332 - val_rmse: 1019.4399\n",
      "Epoch 98/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1115.1801 - rmse: 1115.1810 - val_loss: 1013.8832 - val_rmse: 1019.6038\n",
      "Epoch 99/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1126.0594 - rmse: 1126.0563 - val_loss: 1013.3457 - val_rmse: 1018.9872\n",
      "Epoch 100/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1133.9144 - rmse: 1133.9128 - val_loss: 1013.7894 - val_rmse: 1019.5031\n",
      "Epoch 101/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1129.1379 - rmse: 1129.1387 - val_loss: 1013.7075 - val_rmse: 1019.4114\n",
      "Epoch 102/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1119.5421 - rmse: 1119.5426 - val_loss: 1014.0004 - val_rmse: 1019.7318\n",
      "Epoch 103/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1107.5991 - rmse: 1107.6034 - val_loss: 1013.8347 - val_rmse: 1019.5365\n",
      "Epoch 104/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1119.4493 - rmse: 1119.4504 - val_loss: 1013.9272 - val_rmse: 1019.6465\n",
      "Epoch 105/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1113.4991 - rmse: 1113.4994 - val_loss: 1013.6496 - val_rmse: 1019.3284\n",
      "Epoch 106/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1108.8772 - rmse: 1108.8792 - val_loss: 1013.8278 - val_rmse: 1019.5300\n",
      "Epoch 107/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1129.8623 - rmse: 1129.8589 - val_loss: 1013.8688 - val_rmse: 1019.5785\n",
      "Epoch 108/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1130.1490 - rmse: 1130.1488 - val_loss: 1014.0356 - val_rmse: 1019.7638\n",
      "Epoch 109/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1138.8069 - rmse: 1138.8048 - val_loss: 1013.8468 - val_rmse: 1019.5339\n",
      "Epoch 110/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1141.4384 - rmse: 1141.4425 - val_loss: 1013.7755 - val_rmse: 1019.4604\n",
      "Epoch 111/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1157.8000 - rmse: 1157.8013 - val_loss: 1013.5778 - val_rmse: 1019.2468\n",
      "Epoch 112/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1137.9680 - rmse: 1137.9702 - val_loss: 1013.8706 - val_rmse: 1019.5762\n",
      "Epoch 113/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1098.6978 - rmse: 1098.6992 - val_loss: 1013.8179 - val_rmse: 1019.5142\n",
      "Epoch 114/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1159.6859 - rmse: 1159.6841 - val_loss: 1013.6304 - val_rmse: 1019.3054\n",
      "Epoch 115/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1127.4044 - rmse: 1127.4062 - val_loss: 1013.6085 - val_rmse: 1019.2784\n",
      "Epoch 116/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1154.8220 - rmse: 1154.8201 - val_loss: 1013.5352 - val_rmse: 1019.1976\n",
      "Epoch 117/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1145.6263 - rmse: 1145.6255 - val_loss: 1013.6396 - val_rmse: 1019.3141\n",
      "Epoch 118/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1128.4558 - rmse: 1128.4573 - val_loss: 1013.5103 - val_rmse: 1019.1615\n",
      "Epoch 119/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1119.3114 - rmse: 1119.3250 - val_loss: 1013.5646 - val_rmse: 1019.2247\n",
      "Epoch 120/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1145.5656 - rmse: 1145.5648 - val_loss: 1013.7006 - val_rmse: 1019.3655\n",
      "Epoch 121/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1116.3582 - rmse: 1116.3574 - val_loss: 1013.7669 - val_rmse: 1019.4472\n",
      "Epoch 122/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1109.5927 - rmse: 1109.5896 - val_loss: 1013.7931 - val_rmse: 1019.4857\n",
      "Epoch 123/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1111.5331 - rmse: 1111.5405 - val_loss: 1013.8773 - val_rmse: 1019.5668\n",
      "Epoch 124/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1119.5425 - rmse: 1119.5431 - val_loss: 1014.2773 - val_rmse: 1020.0120\n",
      "Epoch 125/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1111.5386 - rmse: 1111.5402 - val_loss: 1013.5461 - val_rmse: 1019.1954\n",
      "Epoch 126/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1114.5608 - rmse: 1114.5518 - val_loss: 1013.7690 - val_rmse: 1019.4506\n",
      "Epoch 127/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1107.7180 - rmse: 1107.7200 - val_loss: 1013.6879 - val_rmse: 1019.3575\n",
      "Epoch 128/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.0154 - rmse: 1120.0139 - val_loss: 1013.6097 - val_rmse: 1019.2606\n",
      "Epoch 129/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1124.7112 - rmse: 1124.7129 - val_loss: 1013.7767 - val_rmse: 1019.4516\n",
      "Epoch 130/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1110.1512 - rmse: 1110.1545 - val_loss: 1013.7581 - val_rmse: 1019.4359\n",
      "Epoch 131/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1093.3715 - rmse: 1093.3772 - val_loss: 1014.0856 - val_rmse: 1019.8076\n",
      "Epoch 132/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1133.0204 - rmse: 1133.0238 - val_loss: 1014.0016 - val_rmse: 1019.7155\n",
      "Epoch 133/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1122.5742 - rmse: 1122.5717 - val_loss: 1013.8276 - val_rmse: 1019.5159\n",
      "Epoch 134/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1110.9509 - rmse: 1110.9513 - val_loss: 1013.5610 - val_rmse: 1019.2064\n",
      "Epoch 135/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1131.8188 - rmse: 1131.8171 - val_loss: 1013.6918 - val_rmse: 1019.3610\n",
      "Epoch 136/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1136.3407 - rmse: 1136.3433 - val_loss: 1013.6920 - val_rmse: 1019.3620\n",
      "Epoch 137/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1135.3612 - rmse: 1135.3732 - val_loss: 1013.4280 - val_rmse: 1019.0739\n",
      "Epoch 138/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1121.1794 - rmse: 1121.1786 - val_loss: 1013.6837 - val_rmse: 1019.3667\n",
      "Epoch 139/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1126.0010 - rmse: 1126.0017 - val_loss: 1013.7059 - val_rmse: 1019.3868\n",
      "Epoch 140/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1104.6265 - rmse: 1104.6243 - val_loss: 1013.9633 - val_rmse: 1019.6679\n",
      "Epoch 141/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.5303 - rmse: 1120.5281 - val_loss: 1013.8096 - val_rmse: 1019.4921\n",
      "Epoch 142/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1090.3336 - rmse: 1090.3333 - val_loss: 1014.0421 - val_rmse: 1019.7552\n",
      "Epoch 143/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1126.7856 - rmse: 1126.7812 - val_loss: 1013.8008 - val_rmse: 1019.4893\n",
      "Epoch 144/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1115.6133 - rmse: 1115.6193 - val_loss: 1013.7841 - val_rmse: 1019.4734\n",
      "Epoch 145/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1134.2745 - rmse: 1134.2699 - val_loss: 1013.6068 - val_rmse: 1019.2736\n",
      "Epoch 146/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1130.8076 - rmse: 1130.8129 - val_loss: 1013.5847 - val_rmse: 1019.2437\n",
      "Epoch 147/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1123.6749 - rmse: 1123.6803 - val_loss: 1013.7101 - val_rmse: 1019.3947\n",
      "Epoch 148/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1092.4304 - rmse: 1092.4279 - val_loss: 1013.6584 - val_rmse: 1019.3297\n",
      "Epoch 149/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1139.9231 - rmse: 1139.9259 - val_loss: 1013.6285 - val_rmse: 1019.2921\n",
      "Epoch 150/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1111.8174 - rmse: 1111.8157 - val_loss: 1013.6651 - val_rmse: 1019.3423\n",
      "Epoch 151/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1127.5135 - rmse: 1127.5146 - val_loss: 1013.6454 - val_rmse: 1019.3180\n",
      "Epoch 152/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1083.2230 - rmse: 1083.2263 - val_loss: 1013.7861 - val_rmse: 1019.4724\n",
      "Epoch 153/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1096.8026 - rmse: 1096.7990 - val_loss: 1014.0361 - val_rmse: 1019.7547\n",
      "Epoch 154/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1122.3613 - rmse: 1122.3616 - val_loss: 1013.9011 - val_rmse: 1019.6245\n",
      "Epoch 155/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1125.0607 - rmse: 1125.0597 - val_loss: 1013.5955 - val_rmse: 1019.2549\n",
      "Epoch 156/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1150.2128 - rmse: 1150.2108 - val_loss: 1013.5151 - val_rmse: 1019.1720\n",
      "Epoch 157/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1117.0299 - rmse: 1117.0293 - val_loss: 1013.9462 - val_rmse: 1019.6495\n",
      "Epoch 158/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1128.0721 - rmse: 1128.0677 - val_loss: 1013.8348 - val_rmse: 1019.5268\n",
      "Epoch 159/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1127.4858 - rmse: 1127.4889 - val_loss: 1013.8132 - val_rmse: 1019.5086\n",
      "Epoch 160/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1119.5012 - rmse: 1119.5122 - val_loss: 1013.8555 - val_rmse: 1019.5606\n",
      "Epoch 161/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1127.7062 - rmse: 1127.7057 - val_loss: 1013.5704 - val_rmse: 1019.2330\n",
      "Epoch 162/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1138.8699 - rmse: 1138.8684 - val_loss: 1014.0292 - val_rmse: 1019.7505\n",
      "Epoch 163/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1116.0171 - rmse: 1116.0188 - val_loss: 1013.8730 - val_rmse: 1019.5926\n",
      "Epoch 164/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1113.5310 - rmse: 1113.5288 - val_loss: 1013.7110 - val_rmse: 1019.4029\n",
      "Epoch 165/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1115.5135 - rmse: 1115.5118 - val_loss: 1013.5836 - val_rmse: 1019.2661\n",
      "Epoch 166/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1124.7550 - rmse: 1124.7566 - val_loss: 1013.5848 - val_rmse: 1019.2700\n",
      "Epoch 167/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1106.2432 - rmse: 1106.2402 - val_loss: 1013.6528 - val_rmse: 1019.3374\n",
      "Epoch 168/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1099.8279 - rmse: 1099.8312 - val_loss: 1013.7546 - val_rmse: 1019.4595\n",
      "Epoch 169/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1090.4893 - rmse: 1090.4913 - val_loss: 1013.7768 - val_rmse: 1019.4830\n",
      "Epoch 170/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1114.5156 - rmse: 1114.5221 - val_loss: 1013.6601 - val_rmse: 1019.3566\n",
      "Epoch 171/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1121.3513 - rmse: 1121.3406 - val_loss: 1013.7637 - val_rmse: 1019.4786\n",
      "Epoch 172/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1107.4023 - rmse: 1107.4031 - val_loss: 1013.6993 - val_rmse: 1019.3961\n",
      "Epoch 173/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1136.8719 - rmse: 1136.8734 - val_loss: 1013.6260 - val_rmse: 1019.3229\n",
      "Epoch 174/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1125.7878 - rmse: 1125.7904 - val_loss: 1013.6953 - val_rmse: 1019.3964\n",
      "Epoch 175/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1116.8256 - rmse: 1116.8186 - val_loss: 1013.7600 - val_rmse: 1019.4604\n",
      "Epoch 176/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1090.3634 - rmse: 1090.3628 - val_loss: 1014.0135 - val_rmse: 1019.7408\n",
      "Epoch 177/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1106.2821 - rmse: 1106.2919 - val_loss: 1014.1035 - val_rmse: 1019.8486\n",
      "Epoch 178/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1126.5067 - rmse: 1126.5090 - val_loss: 1013.6337 - val_rmse: 1019.3193\n",
      "Epoch 179/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1110.0225 - rmse: 1110.0221 - val_loss: 1013.5113 - val_rmse: 1019.1747\n",
      "Epoch 180/250\n",
      "\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1108.3459 - rmse: 1108.3496 - val_loss: 1013.7089 - val_rmse: 1019.4055\n",
      "Epoch 181/250\n",
      "\u001b[1m 44/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1122.0422 - rmse: 1122.0422 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[553], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks=[reduce_lr],\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    317\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 318\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    320\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32md:\\Anconda_new\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train_encoded, y_train,\n",
    "    validation_data=(X_val_encoded, y_val),\n",
    "    epochs=250,\n",
    "    batch_size=64,\n",
    "    # callbacks=[reduce_lr],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e586dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1108.2147 - rmse: 1108.1992 - val_loss: 1005.2528 - val_rmse: 1003.9106 - learning_rate: 1.0000e-06\n",
      "Epoch 2/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.8899 - rmse: 1120.8779 - val_loss: 1005.3051 - val_rmse: 1003.9420 - learning_rate: 1.0000e-06\n",
      "Epoch 3/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1135.3695 - rmse: 1135.3514 - val_loss: 1005.2919 - val_rmse: 1003.9293 - learning_rate: 1.0000e-06\n",
      "Epoch 4/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1142.3370 - rmse: 1142.3334 - val_loss: 1005.2426 - val_rmse: 1003.8891 - learning_rate: 1.0000e-06\n",
      "Epoch 5/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1129.5983 - rmse: 1129.6219 - val_loss: 1005.3969 - val_rmse: 1004.0538 - learning_rate: 1.0000e-06\n",
      "Epoch 6/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1136.1104 - rmse: 1136.0922 - val_loss: 1005.3150 - val_rmse: 1003.9651 - learning_rate: 1.0000e-06\n",
      "Epoch 7/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1124.8796 - rmse: 1124.8851 - val_loss: 1005.2825 - val_rmse: 1003.9340 - learning_rate: 1.0000e-06\n",
      "Epoch 8/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1099.2781 - rmse: 1099.2574 - val_loss: 1005.3447 - val_rmse: 1004.0036 - learning_rate: 1.0000e-06\n",
      "Epoch 9/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1141.5431 - rmse: 1141.5474 - val_loss: 1005.3116 - val_rmse: 1003.9625 - learning_rate: 1.0000e-06\n",
      "Epoch 10/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1124.3239 - rmse: 1124.3521 - val_loss: 1005.2890 - val_rmse: 1003.9420 - learning_rate: 1.0000e-06\n",
      "Epoch 11/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1100.2745 - rmse: 1100.2646 - val_loss: 1005.2876 - val_rmse: 1003.9537 - learning_rate: 1.0000e-06\n",
      "Epoch 12/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1134.9237 - rmse: 1134.9066 - val_loss: 1005.2890 - val_rmse: 1003.9554 - learning_rate: 1.0000e-06\n",
      "Epoch 13/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1116.3162 - rmse: 1116.3420 - val_loss: 1005.3758 - val_rmse: 1004.0491 - learning_rate: 1.0000e-06\n",
      "Epoch 14/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1111.5139 - rmse: 1111.5127 - val_loss: 1005.3206 - val_rmse: 1003.9820 - learning_rate: 1.0000e-06\n",
      "Epoch 15/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1104.9873 - rmse: 1104.9630 - val_loss: 1005.2707 - val_rmse: 1003.9249 - learning_rate: 1.0000e-06\n",
      "Epoch 16/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1116.5471 - rmse: 1116.5431 - val_loss: 1005.3821 - val_rmse: 1004.0386 - learning_rate: 1.0000e-06\n",
      "Epoch 17/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1137.3092 - rmse: 1137.3021 - val_loss: 1005.3174 - val_rmse: 1003.9784 - learning_rate: 1.0000e-06\n",
      "Epoch 18/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.8813 - rmse: 1123.8884 - val_loss: 1005.3068 - val_rmse: 1003.9626 - learning_rate: 1.0000e-06\n",
      "Epoch 19/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1093.7292 - rmse: 1093.7578 - val_loss: 1005.3317 - val_rmse: 1003.9917 - learning_rate: 1.0000e-06\n",
      "Epoch 20/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1097.1023 - rmse: 1097.0986 - val_loss: 1005.1980 - val_rmse: 1003.8445 - learning_rate: 1.0000e-06\n",
      "Epoch 21/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.4949 - rmse: 1123.4897 - val_loss: 1005.2158 - val_rmse: 1003.8661 - learning_rate: 1.0000e-06\n",
      "Epoch 22/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1093.0775 - rmse: 1093.0677 - val_loss: 1005.2929 - val_rmse: 1003.9543 - learning_rate: 1.0000e-06\n",
      "Epoch 23/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1131.3230 - rmse: 1131.3210 - val_loss: 1005.2715 - val_rmse: 1003.9288 - learning_rate: 1.0000e-06\n",
      "Epoch 24/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.2449 - rmse: 1123.2462 - val_loss: 1005.2161 - val_rmse: 1003.8634 - learning_rate: 1.0000e-06\n",
      "Epoch 25/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1128.5948 - rmse: 1128.5781 - val_loss: 1005.2223 - val_rmse: 1003.8630 - learning_rate: 1.0000e-06\n",
      "Epoch 26/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1143.7356 - rmse: 1143.7208 - val_loss: 1005.3161 - val_rmse: 1003.9731 - learning_rate: 1.0000e-06\n",
      "Epoch 27/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1134.1521 - rmse: 1134.1437 - val_loss: 1005.2949 - val_rmse: 1003.9489 - learning_rate: 1.0000e-06\n",
      "Epoch 28/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1105.3917 - rmse: 1105.4086 - val_loss: 1005.2905 - val_rmse: 1003.9507 - learning_rate: 1.0000e-06\n",
      "Epoch 29/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1101.6802 - rmse: 1101.6742 - val_loss: 1005.3235 - val_rmse: 1003.9890 - learning_rate: 1.0000e-06\n",
      "Epoch 30/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1122.8936 - rmse: 1122.8816 - val_loss: 1005.3246 - val_rmse: 1003.9833 - learning_rate: 1.0000e-06\n",
      "Epoch 31/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1117.5353 - rmse: 1117.5448 - val_loss: 1005.2467 - val_rmse: 1003.8897 - learning_rate: 1.0000e-06\n",
      "Epoch 32/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1135.5593 - rmse: 1135.5475 - val_loss: 1005.2780 - val_rmse: 1003.9304 - learning_rate: 1.0000e-06\n",
      "Epoch 33/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1113.6641 - rmse: 1113.6831 - val_loss: 1005.2906 - val_rmse: 1003.9470 - learning_rate: 1.0000e-06\n",
      "Epoch 34/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1137.7300 - rmse: 1137.7246 - val_loss: 1005.2075 - val_rmse: 1003.8575 - learning_rate: 1.0000e-06\n",
      "Epoch 35/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1135.1375 - rmse: 1135.1112 - val_loss: 1005.2525 - val_rmse: 1003.8882 - learning_rate: 1.0000e-06\n",
      "Epoch 36/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1110.1493 - rmse: 1110.1536 - val_loss: 1005.3589 - val_rmse: 1004.0121 - learning_rate: 1.0000e-06\n",
      "Epoch 37/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1124.9729 - rmse: 1124.9597 - val_loss: 1005.2081 - val_rmse: 1003.8667 - learning_rate: 1.0000e-06\n",
      "Epoch 38/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.2668 - rmse: 1123.3097 - val_loss: 1005.3529 - val_rmse: 1004.0104 - learning_rate: 1.0000e-06\n",
      "Epoch 39/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1096.5081 - rmse: 1096.5134 - val_loss: 1005.3564 - val_rmse: 1004.0062 - learning_rate: 1.0000e-06\n",
      "Epoch 40/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1114.4473 - rmse: 1114.4441 - val_loss: 1005.2906 - val_rmse: 1003.9368 - learning_rate: 1.0000e-06\n",
      "Epoch 41/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1098.1224 - rmse: 1098.1155 - val_loss: 1005.3568 - val_rmse: 1004.0177 - learning_rate: 1.0000e-06\n",
      "Epoch 42/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1112.6936 - rmse: 1112.6862 - val_loss: 1005.2960 - val_rmse: 1003.9468 - learning_rate: 1.0000e-06\n",
      "Epoch 43/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1142.8416 - rmse: 1142.8497 - val_loss: 1005.2582 - val_rmse: 1003.8798 - learning_rate: 1.0000e-06\n",
      "Epoch 44/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1122.6360 - rmse: 1122.6299 - val_loss: 1005.2906 - val_rmse: 1003.9254 - learning_rate: 1.0000e-06\n",
      "Epoch 45/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1121.4597 - rmse: 1121.4490 - val_loss: 1005.2421 - val_rmse: 1003.8846 - learning_rate: 1.0000e-06\n",
      "Epoch 46/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1121.2015 - rmse: 1121.2185 - val_loss: 1005.2676 - val_rmse: 1003.9120 - learning_rate: 1.0000e-06\n",
      "Epoch 47/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1124.7711 - rmse: 1124.7679 - val_loss: 1005.2662 - val_rmse: 1003.9127 - learning_rate: 1.0000e-06\n",
      "Epoch 48/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1119.0771 - rmse: 1119.0626 - val_loss: 1005.2370 - val_rmse: 1003.8851 - learning_rate: 1.0000e-06\n",
      "Epoch 49/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1122.2057 - rmse: 1122.2085 - val_loss: 1005.1666 - val_rmse: 1003.8169 - learning_rate: 1.0000e-06\n",
      "Epoch 50/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1148.2816 - rmse: 1148.2788 - val_loss: 1005.2602 - val_rmse: 1003.9074 - learning_rate: 1.0000e-06\n",
      "Epoch 51/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1098.3066 - rmse: 1098.2887 - val_loss: 1005.1946 - val_rmse: 1003.8411 - learning_rate: 1.0000e-06\n",
      "Epoch 52/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1115.0408 - rmse: 1115.0184 - val_loss: 1005.2033 - val_rmse: 1003.8422 - learning_rate: 1.0000e-06\n",
      "Epoch 53/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1147.1644 - rmse: 1147.2006 - val_loss: 1005.2864 - val_rmse: 1003.9224 - learning_rate: 1.0000e-06\n",
      "Epoch 54/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1121.4551 - rmse: 1121.4508 - val_loss: 1005.3964 - val_rmse: 1004.0588 - learning_rate: 1.0000e-06\n",
      "Epoch 55/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1129.6367 - rmse: 1129.6356 - val_loss: 1005.2861 - val_rmse: 1003.9329 - learning_rate: 1.0000e-06\n",
      "Epoch 56/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1124.2754 - rmse: 1124.2588 - val_loss: 1005.2807 - val_rmse: 1003.9337 - learning_rate: 1.0000e-06\n",
      "Epoch 57/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.7186 - rmse: 1120.7440 - val_loss: 1005.2719 - val_rmse: 1003.9274 - learning_rate: 1.0000e-06\n",
      "Epoch 58/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1133.0309 - rmse: 1133.0151 - val_loss: 1005.4013 - val_rmse: 1004.0527 - learning_rate: 1.0000e-06\n",
      "Epoch 59/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1129.3624 - rmse: 1129.3336 - val_loss: 1005.2480 - val_rmse: 1003.8965 - learning_rate: 1.0000e-06\n",
      "Epoch 60/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1126.1678 - rmse: 1126.1869 - val_loss: 1005.0942 - val_rmse: 1003.7499 - learning_rate: 1.0000e-06\n",
      "Epoch 61/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1146.2118 - rmse: 1146.2141 - val_loss: 1005.2843 - val_rmse: 1003.9335 - learning_rate: 1.0000e-06\n",
      "Epoch 62/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1113.9254 - rmse: 1113.9072 - val_loss: 1005.1307 - val_rmse: 1003.7745 - learning_rate: 1.0000e-06\n",
      "Epoch 63/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1119.0612 - rmse: 1119.0629 - val_loss: 1005.3467 - val_rmse: 1004.0104 - learning_rate: 1.0000e-06\n",
      "Epoch 64/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1132.8138 - rmse: 1132.8043 - val_loss: 1005.2252 - val_rmse: 1003.8782 - learning_rate: 1.0000e-06\n",
      "Epoch 65/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1138.9438 - rmse: 1138.9177 - val_loss: 1005.2526 - val_rmse: 1003.9086 - learning_rate: 1.0000e-06\n",
      "Epoch 66/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.1448 - rmse: 1123.1543 - val_loss: 1005.1863 - val_rmse: 1003.8383 - learning_rate: 1.0000e-06\n",
      "Epoch 67/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1148.8567 - rmse: 1148.8815 - val_loss: 1005.1208 - val_rmse: 1003.7672 - learning_rate: 1.0000e-06\n",
      "Epoch 68/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1125.1462 - rmse: 1125.1392 - val_loss: 1005.1868 - val_rmse: 1003.8223 - learning_rate: 1.0000e-06\n",
      "Epoch 69/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1147.0698 - rmse: 1147.0643 - val_loss: 1005.3057 - val_rmse: 1003.9485 - learning_rate: 1.0000e-06\n",
      "Epoch 70/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1126.9199 - rmse: 1126.9338 - val_loss: 1005.2460 - val_rmse: 1003.8969 - learning_rate: 1.0000e-06\n",
      "Epoch 71/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1077.6029 - rmse: 1077.5880 - val_loss: 1005.1799 - val_rmse: 1003.8351 - learning_rate: 1.0000e-06\n",
      "Epoch 72/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1130.4606 - rmse: 1130.4598 - val_loss: 1005.2377 - val_rmse: 1003.8966 - learning_rate: 1.0000e-06\n",
      "Epoch 73/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1102.0583 - rmse: 1102.0424 - val_loss: 1005.2865 - val_rmse: 1003.9462 - learning_rate: 1.0000e-06\n",
      "Epoch 74/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1145.8483 - rmse: 1145.8491 - val_loss: 1005.2725 - val_rmse: 1003.9034 - learning_rate: 1.0000e-06\n",
      "Epoch 75/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1132.3488 - rmse: 1132.3541 - val_loss: 1005.1942 - val_rmse: 1003.8375 - learning_rate: 1.0000e-06\n",
      "Epoch 76/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1119.6863 - rmse: 1119.6798 - val_loss: 1005.1373 - val_rmse: 1003.7797 - learning_rate: 1.0000e-06\n",
      "Epoch 77/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1125.8513 - rmse: 1125.8607 - val_loss: 1005.1607 - val_rmse: 1003.8094 - learning_rate: 1.0000e-06\n",
      "Epoch 78/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1144.3995 - rmse: 1144.4231 - val_loss: 1005.0692 - val_rmse: 1003.7106 - learning_rate: 1.0000e-06\n",
      "Epoch 79/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1139.5730 - rmse: 1139.5669 - val_loss: 1005.0588 - val_rmse: 1003.6992 - learning_rate: 1.0000e-06\n",
      "Epoch 80/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1112.7572 - rmse: 1112.7562 - val_loss: 1005.2650 - val_rmse: 1003.9174 - learning_rate: 1.0000e-06\n",
      "Epoch 81/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1134.9242 - rmse: 1134.9133 - val_loss: 1005.2473 - val_rmse: 1003.8848 - learning_rate: 1.0000e-06\n",
      "Epoch 82/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1146.9102 - rmse: 1146.8970 - val_loss: 1005.2932 - val_rmse: 1003.9501 - learning_rate: 1.0000e-06\n",
      "Epoch 83/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1140.5016 - rmse: 1140.4862 - val_loss: 1005.2255 - val_rmse: 1003.8613 - learning_rate: 1.0000e-06\n",
      "Epoch 84/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1146.4133 - rmse: 1146.4303 - val_loss: 1005.2545 - val_rmse: 1003.8956 - learning_rate: 1.0000e-06\n",
      "Epoch 85/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1126.0411 - rmse: 1126.0247 - val_loss: 1005.2119 - val_rmse: 1003.8502 - learning_rate: 1.0000e-06\n",
      "Epoch 86/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1105.9197 - rmse: 1105.9247 - val_loss: 1005.2874 - val_rmse: 1003.9534 - learning_rate: 1.0000e-06\n",
      "Epoch 87/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1111.5388 - rmse: 1111.5417 - val_loss: 1005.2805 - val_rmse: 1003.9351 - learning_rate: 1.0000e-06\n",
      "Epoch 88/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1122.3804 - rmse: 1122.3889 - val_loss: 1005.1636 - val_rmse: 1003.8201 - learning_rate: 1.0000e-06\n",
      "Epoch 89/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1111.7606 - rmse: 1111.7659 - val_loss: 1005.2564 - val_rmse: 1003.9047 - learning_rate: 1.0000e-06\n",
      "Epoch 90/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.7828 - rmse: 1120.7800 - val_loss: 1005.2235 - val_rmse: 1003.8788 - learning_rate: 1.0000e-06\n",
      "Epoch 91/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1106.9689 - rmse: 1106.9792 - val_loss: 1005.2665 - val_rmse: 1003.9227 - learning_rate: 1.0000e-06\n",
      "Epoch 92/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1126.2190 - rmse: 1126.2421 - val_loss: 1005.1395 - val_rmse: 1003.7722 - learning_rate: 1.0000e-06\n",
      "Epoch 93/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1137.7992 - rmse: 1137.7988 - val_loss: 1005.1746 - val_rmse: 1003.8177 - learning_rate: 1.0000e-06\n",
      "Epoch 94/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1103.9288 - rmse: 1103.9564 - val_loss: 1005.1617 - val_rmse: 1003.8017 - learning_rate: 1.0000e-06\n",
      "Epoch 95/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1118.2029 - rmse: 1118.1785 - val_loss: 1005.2499 - val_rmse: 1003.8904 - learning_rate: 1.0000e-06\n",
      "Epoch 96/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1131.8495 - rmse: 1131.8462 - val_loss: 1005.2922 - val_rmse: 1003.9457 - learning_rate: 1.0000e-06\n",
      "Epoch 97/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1112.5626 - rmse: 1112.5514 - val_loss: 1005.3893 - val_rmse: 1004.0496 - learning_rate: 1.0000e-06\n",
      "Epoch 98/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1144.7317 - rmse: 1144.7716 - val_loss: 1005.2122 - val_rmse: 1003.8668 - learning_rate: 1.0000e-06\n",
      "Epoch 99/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1121.5352 - rmse: 1121.5515 - val_loss: 1005.2248 - val_rmse: 1003.8797 - learning_rate: 1.0000e-06\n",
      "Epoch 100/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1126.0581 - rmse: 1126.0651 - val_loss: 1005.1938 - val_rmse: 1003.8335 - learning_rate: 1.0000e-06\n",
      "Epoch 101/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1141.8489 - rmse: 1141.8450 - val_loss: 1005.2443 - val_rmse: 1003.9052 - learning_rate: 1.0000e-06\n",
      "Epoch 102/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1113.2428 - rmse: 1113.2345 - val_loss: 1005.1843 - val_rmse: 1003.8464 - learning_rate: 1.0000e-06\n",
      "Epoch 103/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1093.7207 - rmse: 1093.7375 - val_loss: 1005.1393 - val_rmse: 1003.7917 - learning_rate: 1.0000e-06\n",
      "Epoch 104/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.9481 - rmse: 1120.9353 - val_loss: 1005.2164 - val_rmse: 1003.8715 - learning_rate: 1.0000e-06\n",
      "Epoch 105/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1117.7164 - rmse: 1117.7179 - val_loss: 1005.3146 - val_rmse: 1003.9782 - learning_rate: 1.0000e-06\n",
      "Epoch 106/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1143.5924 - rmse: 1143.5858 - val_loss: 1005.2975 - val_rmse: 1003.9395 - learning_rate: 1.0000e-06\n",
      "Epoch 107/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1134.5315 - rmse: 1134.5065 - val_loss: 1005.2875 - val_rmse: 1003.9211 - learning_rate: 1.0000e-06\n",
      "Epoch 108/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1133.0662 - rmse: 1133.0729 - val_loss: 1005.2820 - val_rmse: 1003.9221 - learning_rate: 1.0000e-06\n",
      "Epoch 109/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1119.2517 - rmse: 1119.2244 - val_loss: 1005.2109 - val_rmse: 1003.8403 - learning_rate: 1.0000e-06\n",
      "Epoch 110/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1127.4310 - rmse: 1127.4210 - val_loss: 1005.1336 - val_rmse: 1003.7709 - learning_rate: 1.0000e-06\n",
      "Epoch 111/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1114.6276 - rmse: 1114.6589 - val_loss: 1005.1435 - val_rmse: 1003.7854 - learning_rate: 1.0000e-06\n",
      "Epoch 112/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1110.5928 - rmse: 1110.5854 - val_loss: 1005.2313 - val_rmse: 1003.8697 - learning_rate: 1.0000e-06\n",
      "Epoch 113/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1107.6733 - rmse: 1107.6587 - val_loss: 1005.2897 - val_rmse: 1003.9316 - learning_rate: 1.0000e-06\n",
      "Epoch 114/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1101.7130 - rmse: 1101.7129 - val_loss: 1005.3318 - val_rmse: 1003.9761 - learning_rate: 1.0000e-06\n",
      "Epoch 115/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1111.6511 - rmse: 1111.6595 - val_loss: 1005.2463 - val_rmse: 1003.8766 - learning_rate: 1.0000e-06\n",
      "Epoch 116/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1128.5045 - rmse: 1128.5232 - val_loss: 1005.2308 - val_rmse: 1003.8863 - learning_rate: 1.0000e-06\n",
      "Epoch 117/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1129.7897 - rmse: 1129.7810 - val_loss: 1005.2100 - val_rmse: 1003.8356 - learning_rate: 1.0000e-06\n",
      "Epoch 118/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1131.0093 - rmse: 1131.0128 - val_loss: 1005.2010 - val_rmse: 1003.8488 - learning_rate: 1.0000e-06\n",
      "Epoch 119/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.5225 - rmse: 1123.5133 - val_loss: 1005.1357 - val_rmse: 1003.7797 - learning_rate: 1.0000e-06\n",
      "Epoch 120/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1109.1559 - rmse: 1109.1406 - val_loss: 1005.2197 - val_rmse: 1003.8792 - learning_rate: 1.0000e-06\n",
      "Epoch 121/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1142.4347 - rmse: 1142.4180 - val_loss: 1005.2503 - val_rmse: 1003.9065 - learning_rate: 1.0000e-06\n",
      "Epoch 122/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1135.1393 - rmse: 1135.1157 - val_loss: 1005.3151 - val_rmse: 1003.9582 - learning_rate: 1.0000e-06\n",
      "Epoch 123/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1144.7461 - rmse: 1144.7623 - val_loss: 1005.4360 - val_rmse: 1004.0703 - learning_rate: 1.0000e-06\n",
      "Epoch 124/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1146.3043 - rmse: 1146.3271 - val_loss: 1005.3105 - val_rmse: 1003.9343 - learning_rate: 1.0000e-06\n",
      "Epoch 125/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1134.2355 - rmse: 1134.2726 - val_loss: 1005.2620 - val_rmse: 1003.9077 - learning_rate: 1.0000e-06\n",
      "Epoch 126/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1144.4562 - rmse: 1144.4263 - val_loss: 1005.2824 - val_rmse: 1003.9188 - learning_rate: 1.0000e-06\n",
      "Epoch 127/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1136.0604 - rmse: 1136.0649 - val_loss: 1005.2586 - val_rmse: 1003.9046 - learning_rate: 1.0000e-06\n",
      "Epoch 128/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1100.5627 - rmse: 1100.5546 - val_loss: 1005.2499 - val_rmse: 1003.9055 - learning_rate: 1.0000e-06\n",
      "Epoch 129/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1129.8484 - rmse: 1129.8588 - val_loss: 1005.2180 - val_rmse: 1003.8726 - learning_rate: 1.0000e-06\n",
      "Epoch 130/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1124.9923 - rmse: 1124.9841 - val_loss: 1005.2175 - val_rmse: 1003.8636 - learning_rate: 1.0000e-06\n",
      "Epoch 131/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1086.3297 - rmse: 1086.3370 - val_loss: 1005.1592 - val_rmse: 1003.8095 - learning_rate: 1.0000e-06\n",
      "Epoch 132/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.8175 - rmse: 1123.8237 - val_loss: 1005.1479 - val_rmse: 1003.7937 - learning_rate: 1.0000e-06\n",
      "Epoch 133/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1131.5443 - rmse: 1131.5516 - val_loss: 1005.1844 - val_rmse: 1003.8237 - learning_rate: 1.0000e-06\n",
      "Epoch 134/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1113.6531 - rmse: 1113.6503 - val_loss: 1005.2352 - val_rmse: 1003.8829 - learning_rate: 1.0000e-06\n",
      "Epoch 135/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1137.2845 - rmse: 1137.2877 - val_loss: 1005.3615 - val_rmse: 1003.9954 - learning_rate: 1.0000e-06\n",
      "Epoch 136/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1138.4396 - rmse: 1138.4209 - val_loss: 1005.2802 - val_rmse: 1003.9249 - learning_rate: 1.0000e-06\n",
      "Epoch 137/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1110.5587 - rmse: 1110.5498 - val_loss: 1005.2144 - val_rmse: 1003.8503 - learning_rate: 1.0000e-06\n",
      "Epoch 138/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1134.5771 - rmse: 1134.5828 - val_loss: 1005.2804 - val_rmse: 1003.9031 - learning_rate: 1.0000e-06\n",
      "Epoch 139/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1131.6482 - rmse: 1131.6420 - val_loss: 1005.4487 - val_rmse: 1004.0918 - learning_rate: 1.0000e-06\n",
      "Epoch 140/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1154.8091 - rmse: 1154.8024 - val_loss: 1005.3361 - val_rmse: 1003.9604 - learning_rate: 1.0000e-06\n",
      "Epoch 141/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1107.5680 - rmse: 1107.5652 - val_loss: 1005.3038 - val_rmse: 1003.9350 - learning_rate: 1.0000e-06\n",
      "Epoch 142/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1138.5785 - rmse: 1138.5935 - val_loss: 1005.3163 - val_rmse: 1003.9505 - learning_rate: 1.0000e-06\n",
      "Epoch 143/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1131.4242 - rmse: 1131.4486 - val_loss: 1005.3099 - val_rmse: 1003.9464 - learning_rate: 1.0000e-06\n",
      "Epoch 144/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1143.7186 - rmse: 1143.7141 - val_loss: 1005.2645 - val_rmse: 1003.9105 - learning_rate: 1.0000e-06\n",
      "Epoch 145/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1109.2521 - rmse: 1109.2660 - val_loss: 1005.2853 - val_rmse: 1003.9164 - learning_rate: 1.0000e-06\n",
      "Epoch 146/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1118.5288 - rmse: 1118.5094 - val_loss: 1005.2124 - val_rmse: 1003.8598 - learning_rate: 1.0000e-06\n",
      "Epoch 147/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.8309 - rmse: 1123.8157 - val_loss: 1005.2347 - val_rmse: 1003.8649 - learning_rate: 1.0000e-06\n",
      "Epoch 148/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1137.6167 - rmse: 1137.6083 - val_loss: 1005.1953 - val_rmse: 1003.8297 - learning_rate: 1.0000e-06\n",
      "Epoch 149/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1129.0939 - rmse: 1129.0933 - val_loss: 1005.2540 - val_rmse: 1003.8896 - learning_rate: 1.0000e-06\n",
      "Epoch 150/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1110.8481 - rmse: 1110.8319 - val_loss: 1005.2278 - val_rmse: 1003.8741 - learning_rate: 1.0000e-06\n",
      "Epoch 151/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1144.4906 - rmse: 1144.4803 - val_loss: 1005.1671 - val_rmse: 1003.8105 - learning_rate: 1.0000e-06\n",
      "Epoch 152/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1129.8781 - rmse: 1129.8855 - val_loss: 1005.1872 - val_rmse: 1003.8218 - learning_rate: 1.0000e-06\n",
      "Epoch 153/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1132.6097 - rmse: 1132.6458 - val_loss: 1005.2623 - val_rmse: 1003.8898 - learning_rate: 1.0000e-06\n",
      "Epoch 154/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1097.6721 - rmse: 1097.7101 - val_loss: 1005.2188 - val_rmse: 1003.8674 - learning_rate: 1.0000e-06\n",
      "Epoch 155/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1101.8431 - rmse: 1101.8301 - val_loss: 1005.1406 - val_rmse: 1003.7781 - learning_rate: 1.0000e-06\n",
      "Epoch 156/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1141.3605 - rmse: 1141.3499 - val_loss: 1005.2060 - val_rmse: 1003.8410 - learning_rate: 1.0000e-06\n",
      "Epoch 157/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1114.0144 - rmse: 1114.0165 - val_loss: 1005.2206 - val_rmse: 1003.8682 - learning_rate: 1.0000e-06\n",
      "Epoch 158/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1125.6586 - rmse: 1125.6453 - val_loss: 1005.2703 - val_rmse: 1003.9186 - learning_rate: 1.0000e-06\n",
      "Epoch 159/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1136.0402 - rmse: 1136.0228 - val_loss: 1005.2896 - val_rmse: 1003.9286 - learning_rate: 1.0000e-06\n",
      "Epoch 160/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1136.6671 - rmse: 1136.6710 - val_loss: 1005.1786 - val_rmse: 1003.8173 - learning_rate: 1.0000e-06\n",
      "Epoch 161/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1133.3960 - rmse: 1133.3900 - val_loss: 1005.2526 - val_rmse: 1003.8844 - learning_rate: 1.0000e-06\n",
      "Epoch 162/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1134.1632 - rmse: 1134.1569 - val_loss: 1005.3298 - val_rmse: 1003.9672 - learning_rate: 1.0000e-06\n",
      "Epoch 163/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1135.4863 - rmse: 1135.4684 - val_loss: 1005.1381 - val_rmse: 1003.7769 - learning_rate: 1.0000e-06\n",
      "Epoch 164/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1108.5861 - rmse: 1108.5759 - val_loss: 1005.1455 - val_rmse: 1003.7914 - learning_rate: 1.0000e-06\n",
      "Epoch 165/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1125.5431 - rmse: 1125.5273 - val_loss: 1005.1645 - val_rmse: 1003.8311 - learning_rate: 1.0000e-06\n",
      "Epoch 166/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1128.7542 - rmse: 1128.7572 - val_loss: 1005.3204 - val_rmse: 1003.9938 - learning_rate: 1.0000e-06\n",
      "Epoch 167/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1130.5344 - rmse: 1130.5592 - val_loss: 1005.2072 - val_rmse: 1003.8546 - learning_rate: 1.0000e-06\n",
      "Epoch 168/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1127.8159 - rmse: 1127.8258 - val_loss: 1005.1265 - val_rmse: 1003.7729 - learning_rate: 1.0000e-06\n",
      "Epoch 169/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1112.1301 - rmse: 1112.1593 - val_loss: 1005.3400 - val_rmse: 1004.0031 - learning_rate: 1.0000e-06\n",
      "Epoch 170/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1123.2833 - rmse: 1123.2854 - val_loss: 1005.3429 - val_rmse: 1004.0082 - learning_rate: 1.0000e-06\n",
      "Epoch 171/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1134.0129 - rmse: 1134.0010 - val_loss: 1005.2670 - val_rmse: 1003.9172 - learning_rate: 1.0000e-06\n",
      "Epoch 172/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1143.1277 - rmse: 1143.1379 - val_loss: 1005.3047 - val_rmse: 1003.9710 - learning_rate: 1.0000e-06\n",
      "Epoch 173/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1091.6626 - rmse: 1091.6520 - val_loss: 1005.1586 - val_rmse: 1003.8000 - learning_rate: 1.0000e-06\n",
      "Epoch 174/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1133.6039 - rmse: 1133.5975 - val_loss: 1005.1685 - val_rmse: 1003.8005 - learning_rate: 1.0000e-06\n",
      "Epoch 175/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1126.0620 - rmse: 1126.0486 - val_loss: 1005.2117 - val_rmse: 1003.8511 - learning_rate: 1.0000e-06\n",
      "Epoch 176/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1113.6527 - rmse: 1113.6499 - val_loss: 1005.2566 - val_rmse: 1003.8976 - learning_rate: 1.0000e-06\n",
      "Epoch 177/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1131.0739 - rmse: 1131.0983 - val_loss: 1005.1400 - val_rmse: 1003.7719 - learning_rate: 1.0000e-06\n",
      "Epoch 178/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1129.8538 - rmse: 1129.8628 - val_loss: 1005.0601 - val_rmse: 1003.7022 - learning_rate: 1.0000e-06\n",
      "Epoch 179/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1119.7173 - rmse: 1119.7596 - val_loss: 1005.2120 - val_rmse: 1003.8730 - learning_rate: 1.0000e-06\n",
      "Epoch 180/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1114.1271 - rmse: 1114.1387 - val_loss: 1005.2099 - val_rmse: 1003.8511 - learning_rate: 1.0000e-06\n",
      "Epoch 181/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1147.7263 - rmse: 1147.7677 - val_loss: 1005.2109 - val_rmse: 1003.8665 - learning_rate: 1.0000e-06\n",
      "Epoch 182/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1144.4818 - rmse: 1144.5076 - val_loss: 1005.2068 - val_rmse: 1003.8613 - learning_rate: 1.0000e-06\n",
      "Epoch 183/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1121.6917 - rmse: 1121.7339 - val_loss: 1005.1841 - val_rmse: 1003.8259 - learning_rate: 1.0000e-06\n",
      "Epoch 184/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1116.3311 - rmse: 1116.3195 - val_loss: 1005.3256 - val_rmse: 1003.9691 - learning_rate: 1.0000e-06\n",
      "Epoch 185/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1121.0034 - rmse: 1120.9893 - val_loss: 1005.2084 - val_rmse: 1003.8458 - learning_rate: 1.0000e-06\n",
      "Epoch 186/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1105.2578 - rmse: 1105.2632 - val_loss: 1005.3083 - val_rmse: 1003.9660 - learning_rate: 1.0000e-06\n",
      "Epoch 187/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1120.2369 - rmse: 1120.2390 - val_loss: 1005.2262 - val_rmse: 1003.8777 - learning_rate: 1.0000e-06\n",
      "Epoch 188/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1116.3894 - rmse: 1116.3925 - val_loss: 1005.2612 - val_rmse: 1003.9116 - learning_rate: 1.0000e-06\n",
      "Epoch 189/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1094.8475 - rmse: 1094.8840 - val_loss: 1005.2300 - val_rmse: 1003.8641 - learning_rate: 1.0000e-06\n",
      "Epoch 190/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1133.1720 - rmse: 1133.1759 - val_loss: 1005.1462 - val_rmse: 1003.7716 - learning_rate: 1.0000e-06\n",
      "Epoch 191/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1119.6644 - rmse: 1119.6815 - val_loss: 1005.2176 - val_rmse: 1003.8491 - learning_rate: 1.0000e-06\n",
      "Epoch 192/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1133.7921 - rmse: 1133.7817 - val_loss: 1005.1738 - val_rmse: 1003.8024 - learning_rate: 1.0000e-06\n",
      "Epoch 193/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1098.9325 - rmse: 1098.9623 - val_loss: 1005.1200 - val_rmse: 1003.7539 - learning_rate: 1.0000e-06\n",
      "Epoch 194/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1154.2015 - rmse: 1154.1854 - val_loss: 1005.2685 - val_rmse: 1003.8942 - learning_rate: 1.0000e-06\n",
      "Epoch 195/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1121.1503 - rmse: 1121.1748 - val_loss: 1005.1270 - val_rmse: 1003.7673 - learning_rate: 1.0000e-06\n",
      "Epoch 196/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1144.5071 - rmse: 1144.4839 - val_loss: 1005.2446 - val_rmse: 1003.8767 - learning_rate: 1.0000e-06\n",
      "Epoch 197/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1114.9371 - rmse: 1114.9431 - val_loss: 1005.1975 - val_rmse: 1003.8372 - learning_rate: 1.0000e-06\n",
      "Epoch 198/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1115.9760 - rmse: 1115.9581 - val_loss: 1005.1337 - val_rmse: 1003.7704 - learning_rate: 1.0000e-06\n",
      "Epoch 199/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1134.5338 - rmse: 1134.5511 - val_loss: 1005.2369 - val_rmse: 1003.8913 - learning_rate: 1.0000e-06\n",
      "Epoch 200/200\n",
      "\u001b[1m107/107\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1100.9294 - rmse: 1100.9353 - val_loss: 1005.0596 - val_rmse: 1003.6896 - learning_rate: 1.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# # early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "# history = model.fit(\n",
    "#     X_train_scaled, y_train,\n",
    "#     validation_data=(X_val_scaled, y_val),\n",
    "#     epochs=200,\n",
    "#     batch_size=64,\n",
    "#     callbacks=[reduce_lr],\n",
    "#     verbose=1\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "49d650bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m178/178\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 967us/step\n"
     ]
    }
   ],
   "source": [
    "test_preds_ann = model.predict(X_test_encoded).flatten()\n",
    "submission_ann = pd.DataFrame({\n",
    "    'Item_Identifier': test['Item_Identifier'],\n",
    "    'Outlet_Identifier': test['Outlet_Identifier'],\n",
    "    'Item_Outlet_Sales': test_preds_ann\n",
    "})\n",
    "submission_ann.to_csv(\"submission_ann.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a57c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
